{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding Assignment 3\n",
    "\n",
    "CS 598 Practical Statistical Learning\n",
    "\n",
    "2023-10-09\n",
    "\n",
    "UIUC Fall 2023\n",
    "\n",
    "**Authors**\n",
    "* Ryan Fogle\n",
    "    - rsfogle2@illinois.edu\n",
    "    - UIN: 652628818\n",
    "* Sean Enright\n",
    "    - seanre2@illinois.edu\n",
    "    - UIN: 661791377\n",
    "\n",
    "**Contributions**\n",
    "\n",
    "Sean contributed to Parts I and II and reviewed Part III, Ryan Contributed to Part III and reviewed Parts I and II. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Optimal span for LOESS\n",
    "\n",
    "Here we implement LOO-CV and GCV to select the optimal span for LOESS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1 imports\n",
    "from csaps import csaps\n",
    "from skmisc.loess import loess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define `onestep_cv()`, which calculates the LOO-CV and GCV values for a given span value, and `find_cv_vals()`, which returns these for all provided span values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onestep_cv(x, y, sp):\n",
    "    \"\"\"Calculate the LOO-CV and GCV for a given span\"\"\"\n",
    "    # 1) Fit a LOESS model y - x with span and extract the\n",
    "    #    corresponding residual vector\n",
    "    loess_fit = loess(x, y, span=sp)\n",
    "    y_hat = loess_fit.predict(x).values\n",
    "    # 2) Call lo_lev to obtain the diagonal entries of S\n",
    "    s_ii = loess_fit.outputs.diagonal\n",
    "    # 3) Compute LOO-CV and GCV\n",
    "    # LOOCV\n",
    "    loocv = np.mean(np.power((y - y_hat) / (1 - s_ii), 2))\n",
    "    # GCV\n",
    "    m = np.mean(s_ii)\n",
    "    gcv = np.mean(np.power((y - y_hat) / (1 - m), 2))\n",
    "    return loocv, gcv\n",
    "\n",
    "def find_cv_vals(x, y, span):\n",
    "    \"\"\"Find LOO-CV and GCV for all provided span values\"\"\"\n",
    "    m = len(span)\n",
    "    cv = np.zeros(m)\n",
    "    gcv = np.zeros(m)\n",
    "\n",
    "    for i in range(m):\n",
    "        cv_i, gcv_i = onestep_cv(x, y, span[i])\n",
    "        cv[i] = cv_i\n",
    "        gcv[i] = gcv_i\n",
    "    return cv, gcv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determining span values that produce the lowest LOOCV and GCV error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://liangfgithub.github.io/Data/Coding3_Data.csv\n",
    "data_part1 = pd.read_csv(\"Coding3_Data.csv\")\n",
    "span_vec = np.linspace(0.2, 0.9, 15)\n",
    "\n",
    "# Find optimal span by LOOCV and GCV\n",
    "loo, gcv = find_cv_vals(data_part1[\"x\"], data_part1[\"y\"], span_vec)\n",
    "\n",
    "# Display table of CV results\n",
    "print(\"Span    LOOCV   GCV\")\n",
    "for s, l, g in zip(span_vec, loo, gcv):\n",
    "    print(f\"{s:.2f}\\t{l:.3f}\\t{g:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The span optimization results are presented in the chart below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "mpl.rcParams['figure.dpi'] = 300\n",
    "plt.scatter(span_vec, loo, color=\"darkorange\", s=5, label=\"LOO-CV\")\n",
    "plt.plot(span_vec, loo, color=\"orange\", alpha=1, linestyle=\"dotted\")\n",
    "plt.scatter(span_vec, gcv, color=\"blue\", s=5, label=\"GCV\")\n",
    "plt.plot(span_vec, gcv, color=\"lightblue\", alpha=0.75, linestyle=\"--\")\n",
    "plt.xlabel(\"Span\")\n",
    "plt.ylabel(\"CV Error\")\n",
    "plt.title(\"Span vs CV Error\")\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this dataset and choice of span values, the best span value selected by LOOCV and GCV is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select lowest span value\n",
    "span_loocv = span_vec[np.argmin(loo)]\n",
    "span_gcv = span_vec[np.argmin(gcv)]\n",
    "\n",
    "print(f\"Span by LOO-CV: {span_loocv}\")\n",
    "print(f\"   Span by GCV: {span_gcv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The true curve is defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return np.sin(12 * (x + 0.2)) / (x + 0.2)\n",
    "\n",
    "fx = np.linspace(min(data_part1[\"x\"]), max(data_part1[\"x\"]), 1001)\n",
    "fy = f(fx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we compare the LOESS curve with LOO-CV and GCV optimized span to the true curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_loess = loess(data_part1[\"x\"], data_part1[\"y\"], span=span_loocv).predict(fx).values\n",
    "\n",
    "sns.set()\n",
    "mpl.rcParams['figure.dpi'] = 300\n",
    "plt.scatter(data_part1[\"x\"], data_part1[\"y\"], color=\"red\", s=6)\n",
    "plt.plot(fx, fy, color=\"gray\", linewidth=1, label=\"True Function\")\n",
    "plt.plot(fx, y_loess, color=\"blue\", linewidth=1, linestyle=\"--\", label=\"LOESS Fit\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Clustering time series\n",
    "\n",
    "In this exercise, we cluster time series data, comparing the results of clustering with and without natural cubic splines.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following imports are specific to Part II."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import inv\n",
    "from scipy.interpolate import splev\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random seed is set to ensure repeatability. The seed is the sum of our UINs. We add 1 to the sum to give a more favorable seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed to the last four digits of our UINs\n",
    "np.random.seed(8818 + 1377 + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we use the Sales_Transactions_Dataset_Weekly dataset from the UCI Machine Learning Repository. After reading the dataset file, we select the time series data and center the time series data by its row means, resulting in $\\textbf{X}_{811 \\times 52}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://archive.ics.uci.edu/dataset/396/sales+transactions+dataset+weekly\n",
    "X = pd.read_csv(\"Sales_Transactions_Dataset_Weekly.csv\",\n",
    "                         index_col=0, usecols=range(53))\n",
    "# Normalize each time series, i.e., normalize each row by its mean\n",
    "X = X.sub(X.mean(axis=1), axis=0).to_numpy()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time series features are simply a vector of indeces corresponding to the weeks of the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.arange(start=1, stop=53)\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, a Python implementation of R's `splines::ns` is provided. It will be used to generate a natural cubic spline basis function matrix. It was copied from [the course website](https://liangfgithub.github.io/Python_W5_RegressionSpline.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref: https://liangfgithub.github.io/Python_W5_RegressionSpline.html\n",
    "# converted from R's ns()\n",
    "def ns(x, df=None, knots=None, boundary_knots=None, include_intercept=False):\n",
    "    degree = 3\n",
    "    \n",
    "    if boundary_knots is None:\n",
    "        boundary_knots = [np.min(x), np.max(x)]\n",
    "    else:\n",
    "        boundary_knots = np.sort(boundary_knots).tolist()\n",
    "\n",
    "    oleft = x < boundary_knots[0]\n",
    "    oright = x > boundary_knots[1]\n",
    "    outside = oleft | oright\n",
    "    inside = ~outside\n",
    "\n",
    "    if df is not None:\n",
    "        nIknots = df - 1 - include_intercept\n",
    "        if nIknots < 0:\n",
    "            nIknots = 0\n",
    "            \n",
    "        if nIknots > 0:\n",
    "            knots = np.linspace(0, 1, num=nIknots + 2)[1:-1]\n",
    "            knots = np.quantile(x[~outside], knots)\n",
    "\n",
    "    Aknots = np.sort(np.concatenate((boundary_knots * 4, knots)))\n",
    "    n_bases = len(Aknots) - (degree + 1)\n",
    "\n",
    "    if any(outside):\n",
    "        basis = np.empty((x.shape[0], n_bases), dtype=float)\n",
    "        e = 1 / 4 # in theory anything in (0, 1); was (implicitly) 0 in R <= 3.2.2\n",
    "\n",
    "        if any(oleft):\n",
    "            k_pivot = boundary_knots[0]\n",
    "            xl = x[oleft] - k_pivot\n",
    "            xl = np.c_[np.ones(xl.shape[0]), xl]\n",
    "\n",
    "            # equivalent to splineDesign(Aknots, rep(k.pivot, ord), ord, derivs)\n",
    "            tt = np.empty((xl.shape[1], n_bases), dtype=float)\n",
    "            for j in range(xl.shape[1]):\n",
    "                for i in range(n_bases):\n",
    "                    coefs = np.zeros((n_bases,))\n",
    "                    coefs[i] = 1\n",
    "                    tt[j, i] = splev(k_pivot, (Aknots, coefs, degree), der=j)\n",
    "\n",
    "            basis[oleft, :] = xl @ tt\n",
    "\n",
    "        if any(oright):\n",
    "            k_pivot = boundary_knots[1]\n",
    "            xr = x[oright] - k_pivot\n",
    "            xr = np.c_[np.ones(xr.shape[0]), xr]\n",
    "\n",
    "            tt = np.empty((xr.shape[1], n_bases), dtype=float)\n",
    "            for j in range(xr.shape[1]):\n",
    "                for i in range(n_bases):\n",
    "                    coefs = np.zeros((n_bases,))\n",
    "                    coefs[i] = 1\n",
    "                    tt[j, i] = splev(k_pivot, (Aknots, coefs, degree), der=j)\n",
    "                    \n",
    "            basis[oright, :] = xr @ tt\n",
    "        \n",
    "        if any(inside):\n",
    "            xi = x[inside]\n",
    "            tt = np.empty((len(xi), n_bases), dtype=float)\n",
    "            for i in range(n_bases):\n",
    "                coefs = np.zeros((n_bases,))\n",
    "                coefs[i] = 1\n",
    "                tt[:, i] = splev(xi, (Aknots, coefs, degree))\n",
    "\n",
    "            basis[inside, :] = tt\n",
    "    else:\n",
    "        basis = np.empty((x.shape[0], n_bases), dtype=float)\n",
    "        for i in range(n_bases):\n",
    "            coefs = np.zeros((n_bases,))\n",
    "            coefs[i] = 1\n",
    "            basis[:, i] = splev(x, (Aknots, coefs, degree))\n",
    "\n",
    "    const = np.empty((2, n_bases), dtype=float)\n",
    "    for i in range(n_bases):\n",
    "        coefs = np.zeros((n_bases,))\n",
    "        coefs[i] = 1\n",
    "        const[:, i] = splev(boundary_knots, (Aknots, coefs, degree), der=2)\n",
    "\n",
    "    if include_intercept is False:\n",
    "        basis = basis[:, 1:]\n",
    "        const = const[:, 1:]\n",
    "\n",
    "    qr_const = np.linalg.qr(const.T, mode='complete')[0]\n",
    "    basis = (qr_const.T @ basis.T).T[:, 2:]\n",
    "\n",
    "    return basis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `ns` to generate the basis function matrix $\\textbf{F}_{52 \\times 9}$ and remove the intercept by centering its columns by their means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_mat = ns(t, df=9, include_intercept=False)\n",
    "F_mat = F_mat - F_mat.mean(axis=0)[np.newaxis, :]\n",
    "F_mat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With $\\textbf{F}$ and $\\textbf{X}$, we can calculate a matrix of spline coefficients for every obbservation, $\\textbf{B}_{811 \\times 9}$. This is found by the following formula, which is implemented below.\n",
    "\n",
    "$$\n",
    "\\textbf{B}^{\\top} = (\\textbf{F}^{\\top} \\ \\textbf{F})^{-1} \\ \\textbf{F}^{\\top} \\textbf{X}^{\\top}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = inv(F_mat.T @ F_mat) @ F_mat.T @ X.T\n",
    "B = B.T\n",
    "B.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering using Matrix $\\textbf{B}$\n",
    "\n",
    "Here we cluster by $\\textbf{B}$, the spline coefficients for each observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 6\n",
    "n_row = 2\n",
    "n_col = 3\n",
    "\n",
    "km_B = KMeans(n_clusters=n_clusters, n_init=10).fit(B)\n",
    "centers_B = F_mat @ km_B.cluster_centers_.T  \n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=3, dpi=300,\n",
    "                        sharex=\"all\", sharey=\"all\")\n",
    "for i in range(n_row):\n",
    "    for j in range(n_col):\n",
    "        series = X[km_B.labels_ == i * n_col + j, :]\n",
    "        for k in range(series.shape[0]):\n",
    "            axs[i, j].plot(t, series[k], color=\"darkgrey\", linewidth=0.75)\n",
    "        axs[i, j].plot(t, centers_B[:, i * n_col + j], color=\"red\", linewidth=0.75)\n",
    "        axs[i, j].set_xlim([1, 52])\n",
    "        axs[i, j].set_ylim([-30, 30])\n",
    "        axs[i, j].set_xticks(range(0, 52, 10))\n",
    "        axs[i, j].set_yticks(np.linspace(-30, 30, 7))\n",
    "        axs[i, j].set_xlabel(\"Weeks\")\n",
    "        axs[i, j].set_ylabel(\"Weekly Sales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering using Matrix $\\textbf{X}$\n",
    "\n",
    "By comparison, we cluster by $\\textbf{X}$, the raw time series data. The centers are noticeably less smooth than the NCS-clustered centers from matrix $\\textbf{B}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_X = KMeans(n_clusters=n_clusters, n_init=10).fit(X)\n",
    "centers_X = km_X.cluster_centers_.T\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=3, dpi=300,\n",
    "                        sharex=\"all\", sharey=\"all\")\n",
    "for i in range(n_row):\n",
    "    for j in range(n_col):\n",
    "        series = X[km_X.labels_ == i * n_col + j, :]\n",
    "        for k in range(series.shape[0]):\n",
    "            axs[i, j].plot(t, series[k], color=\"darkgrey\", linewidth=0.75)\n",
    "        axs[i, j].plot(t, centers_X[:, i * n_col + j], color=\"red\", linewidth=0.75)\n",
    "        axs[i, j].set_xlim([1, 52])\n",
    "        axs[i, j].set_ylim([-30, 30])\n",
    "        axs[i, j].set_xticks(range(0, 52, 10))\n",
    "        axs[i, j].set_yticks(np.linspace(-30, 30, 7))\n",
    "        axs[i, j].set_xlabel(\"Weeks\")\n",
    "        axs[i, j].set_ylabel(\"Weekly Sales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III: Ridgeless and double descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set random seed to the last four digits of our UINs\n",
    "np.random.seed(8818 + 1377 + 1)\n",
    "\n",
    "# read in data\n",
    "df = pd.read_csv('Coding3_dataH.csv', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Ridgeless Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the beta coeficients, we can simplify OLS due to the properties of SVD.\n",
    "\n",
    "We know OLS is closed-form and can be solved via linear algebra by solving for $\\hat{\\beta}$\n",
    "$$\n",
    "y = F \\hat{\\beta}\n",
    "$$\n",
    "\n",
    "Multiple both sides by $F^T$\n",
    "$$\n",
    "F^T y = F^T F \\hat{\\beta}\n",
    "$$\n",
    "\n",
    "Due to SVD, we have the following equations:\n",
    "$$\n",
    "F = U D\n",
    "$$\n",
    "\n",
    "We can now simplify $ F^T F$\n",
    "$$\n",
    "F^T F = (U D)^T (U D) = D^T U^T U D = D^T D\n",
    "$$\n",
    "\n",
    "Now we have:\n",
    "$$\n",
    "F^T y = (D^T D) \\hat{\\beta}\n",
    "$$\n",
    "\n",
    "Then finally:\n",
    "$$\n",
    "\\hat{\\beta} = (D^T D)^{-1} F^T y\n",
    "$$\n",
    "\n",
    "Since $D$ is a diagnonal matrix, $D^T D$ is the same as squaring all of the diagonal entries. The inverse of a diagonal matrix is just $1/d_{ii}$ if $d_{ii}$ are the diagonal entries. So we can preform simple matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCR:\n",
    "    \"\"\"Class to handle Principle Component Regression\n",
    "    \"\"\"\n",
    "    def __init__(self, eps=1e-10):\n",
    "        self.eps = eps\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"Fit model given X (n,m) and y (n,) array\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): Data matrix\n",
    "            y (np.ndarray): Response vector\n",
    "        \"\"\"\n",
    "\n",
    "        # Preform SVD\n",
    "        U, S, Vh = np.linalg.svd(X, full_matrices=False)\n",
    "\n",
    "        # Find PC below threshold\n",
    "        k = (S > self.eps).sum()\n",
    "\n",
    "        # Create new design matrix\n",
    "        F = U @ np.diag(S)\n",
    "\n",
    "        # ignore small PC\n",
    "        F = F[:, :k]\n",
    "\n",
    "        # compute 1/ (singular value)**2 for each singular value\n",
    "        D = np.diag( 1 / S[:k]**2)\n",
    "\n",
    "        # create beta vector, center y \n",
    "        self.beta = D @ F.T @ (y - y.mean()).reshape(-1, 1)\n",
    "\n",
    "        # create intercept\n",
    "        self.b0 = y.mean()\n",
    "        \n",
    "        # save for later use\n",
    "        self.Vh = Vh\n",
    "        self.k = k\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Produce predictions for X (n,m) data matrix\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): Data matrix\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Returns (n,1) predictions\n",
    "        \"\"\"\n",
    "        # compute design matrix\n",
    "        F = X @ self.Vh.T\n",
    "\n",
    "        # remove small PCs\n",
    "        F = F[:, :self.k]\n",
    "        \n",
    "        # compute predictions\n",
    "        return F @ self.beta + self.b0\n",
    "    \n",
    "\n",
    "def ridgeless_sim(X_train, X_test, y_train, y_test, eps):\n",
    "    pcr = PCR(eps=eps)\n",
    "    pcr.fit(X_train, y_train)\n",
    "    y_test_pred = pcr.predict(X_test)\n",
    "    y_train_pred = pcr.predict(X_train)\n",
    "\n",
    "    return (\n",
    "            np.mean((y_train - y_train_pred.reshape(-1))**2), \n",
    "            np.mean((y_test - y_test_pred.reshape(-1))**2)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task II: Simulation Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "T = 30 # number of iterations\n",
    "eps=1e-10 # ignore PC below this number\n",
    "\n",
    "data = []\n",
    "for i in tqdm(range(T)):\n",
    "    X = df.iloc[:, 1:].to_numpy()\n",
    "    y = df.iloc[:, 0].to_numpy()\n",
    "\n",
    "    # center X\n",
    "    X = X - X.mean()\n",
    "    \n",
    "    # create a fit with 5 to 240 features.\n",
    "    for d in range(5, 241):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X[:, :d], y, test_size=0.75)\n",
    "        train_loss, test_loss = ridgeless_sim(X_train, X_test, y_train, y_test, eps)\n",
    "        data.append((i, d, train_loss, test_loss, np.log(test_loss)))\n",
    "results = pd.DataFrame(data,\n",
    "                       columns=['Iteration', '# of Features', 'Training Error',\n",
    "                                'Test Error', 'Log of Test Error'])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = results.groupby('# of Features')[\n",
    "    ['Training Error', 'Test Error', 'Log of Test Error']].median().reset_index()\n",
    "plt.figure()\n",
    "sns.scatterplot(dd, x='# of Features', y='Log of Test Error')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
