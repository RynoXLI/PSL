{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding Assignment 4\n",
    "\n",
    "CS 598 Practical Statistical Learning\n",
    "\n",
    "2023-11-06\n",
    "\n",
    "UIUC Fall 2023\n",
    "\n",
    "**Authors**\n",
    "* Ryan Fogle\n",
    "    - rsfogle2@illinois.edu\n",
    "    - UIN: 652628818\n",
    "* Sean Enright\n",
    "    - seanre2@illinois.edu\n",
    "    - UIN: 661791377\n",
    "\n",
    "**Contributions**\n",
    "\n",
    "Part I:\n",
    "- Ryan contributed to implementing the E-step, Sean contributed to refactoring and completely implementing the EM algorithm.\n",
    "\n",
    "\n",
    "Part II:\n",
    "- Sean implemented the Baum-Welch Algorithm, Ryan implemented the Viterbi Algorithm. \n",
    "\n",
    "## Part 1: Gaussian Mixtures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Functions\n",
    "\n",
    "Here we implement the EM algorithm for a $p$-dimensional Gaussian misxture model with $G$ classes.\n",
    "\n",
    "In the E-step, we calculate the responsibility matrix $r_{nk}$, which represents the probability of a hidden state belonging to a particular class, given the observed data and current Gaussian distribution parameters.\n",
    "\n",
    "$$\n",
    "p(z_n = k \\mid x_n, \\theta^{old}) =\n",
    "r_{nk} =\n",
    "\\frac{\\pi_k \\, \\mathcal{N}(x_n \\mid \\mu_k, \\Sigma_k)}\n",
    "     {\\Sigma_j \\pi_j \\, \\mathcal{N}(x_n \\mid \\mu_j, \\Sigma_j)}\n",
    "$$\n",
    "\n",
    "where $\\mathcal{N}$ is defined as\n",
    "$$\n",
    "\\mathcal{N}(x \\mid \\mu, \\Sigma) =\n",
    "\\frac{1}\n",
    "     {(2\\pi)^{p/2} \\, |\\Sigma|^{1/2}} \\,\n",
    "     \\textrm{exp}[-\\frac{1}{2}(x-\\mu)^{T} \\, |\\Sigma|^{-1} (x-\\mu)]\n",
    "$$\n",
    "\n",
    "and $\\pi_k$ is the mixture weights for the $k$ classes, $\\mu_k$ is the mean of each class and $\\Sigma$ is the shared covariance matrix.\n",
    "\n",
    "In the M-step, we update $\\pi_k$, $\\mu_k$ and $\\Sigma$.\n",
    "\n",
    "$$\\pi_k = \\frac{1}{N}\\sum_n r_{nk}$$\n",
    "\n",
    "$$\\mu_k^{new} = \\frac{\\sum_n r_{nk} \\, x_n} {\\sum_n r_{nk}}$$\n",
    "\n",
    "$$\n",
    "\\Sigma = \\sum_k \\pi_k \\frac{\\sum_n r_{nk} (x_n - \\mu_k^{new}) (x_n - \\mu_k^{new})^T}\n",
    "                           {\\sum_n r_{nk}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Estep(x: pd.DataFrame, G: int, pi: np.ndarray, mu: pd.DataFrame, sigma: pd.DataFrame):\n",
    "    \"\"\"EM algorithm expectation step. Here we estimate the latent variables based on the previous\n",
    "    estimates of theta to build a responsibility matrix.\n",
    "\n",
    "    Args:\n",
    "        x (pd.DataFrame): Data matrix, (n, p)\n",
    "        G (int): Number of classes\n",
    "        pi (np.ndarray): Mixing weights, (G,)\n",
    "        mu (pd.DataFrame): Mean values for each class, (p, G)\n",
    "        sigma (pd.DataFrame): Shared covariance matrix, (p, p)\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: The responsibility matrix of shape (n, G)\n",
    "    \"\"\"\n",
    "    resp = np.zeros((x.shape[0], G))\n",
    "    for k in range(G):\n",
    "        resp[:, k] = pi[k] * multivariate_normal_density(x, mu.iloc[:, k], sigma)\n",
    "    return resp / resp.sum(axis=1).reshape(-1, 1)\n",
    "\n",
    "def multivariate_normal_density(x: pd.DataFrame, mu_k: pd.DataFrame, sigma: pd.DataFrame):\n",
    "    \"\"\"Evaluate multivariate normal probability density.\n",
    "    This is used in the E-step and in the log-likelihood calculation.\n",
    "\n",
    "    Args:\n",
    "        x (pd.DataFrame): data, shape (n, p)\n",
    "        mu_k (pd.DataFrame): mean for a given class, shape (p,)\n",
    "        sigma (pd.DataFrame): covariance matrix, shape (p, p)\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: n-dimensional probability densities\n",
    "    \"\"\"\n",
    "    A_mu = (x.T - mu_k.values.reshape(-1, 1)).to_numpy()\n",
    "    exponent = - 0.5 * np.multiply(A_mu, np.linalg.inv(sigma) @ A_mu).sum(axis=0)\n",
    "    return 1 / (2 * np.pi * np.sqrt(np.linalg.det(sigma))) * np.exp(exponent)\n",
    "\n",
    "def Mstep(x: pd.DataFrame, G: int, resp: np.ndarray, sigma: pd.DataFrame):\n",
    "    \"\"\"EM algorithm maximization step.\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray): Data matrix, (n, p)\n",
    "        G (int): Number of classes\n",
    "        resp (np.ndarray): Responsibility matrix, (n, G)\n",
    "        sigma (pd.DataFrame): Shared covariance matrix, (p, p)\n",
    "    \n",
    "    Returns:\n",
    "        pi_new (np.ndarray): Updated mixing weights, (G,)\n",
    "        mu_new (pd.DataFrame): Updated mean values per dimension, (p, G)\n",
    "        sigma_new (pd.DataFrame): Updated covariance matrix, (p, p)\n",
    "    \"\"\"\n",
    "    n = x.shape[0]\n",
    "    # Pi\n",
    "    pi_new = resp.sum(axis=0) / n\n",
    "    # Mu\n",
    "    mu_new = (x.T @ resp) / resp.sum(axis=0)\n",
    "    # Sigma\n",
    "    sigma_new = sigma.copy()\n",
    "    sigma_new.values[:, :] = 0\n",
    "    for k in range(G):\n",
    "        tmp = x.T - mu_new.values[:, k].reshape(-1, 1)\n",
    "        tmp = tmp.to_numpy()\n",
    "        sigma_new += pi_new[k] * tmp @ np.diag(resp[:, k]) @ tmp.T / resp[:, k].sum()\n",
    "    return pi_new, mu_new, sigma_new\n",
    "\n",
    "def loglik(x: pd.DataFrame, G: int, pi: np.ndarray, mu: pd.DataFrame, sigma: pd.DataFrame):\n",
    "    \"\"\"Calculate log likelihood, given distribution parameters.\n",
    "\n",
    "    Args:\n",
    "        x (pd.DataFrame): Input data, shape (n, p)\n",
    "        G (int): Number of classes\n",
    "        pi (np.ndarray): Mixing weights, shape (G,)\n",
    "        mu (pd.DataFrame): Distribution means for each class, shape (p, G)\n",
    "        sigma (pd.DataFrame): Shared covariance matrix, shape (p, p)\n",
    "\n",
    "    Returns:\n",
    "        float: log likelihood\n",
    "    \"\"\"\n",
    "    ll = np.zeros(x.shape[0])\n",
    "    for k in range(G):\n",
    "        ll += pi[k] * multivariate_normal_density(x, mu.iloc[:, k], sigma)\n",
    "    return np.log(ll).sum()\n",
    "\n",
    "def myEM(data: pd.DataFrame, G: int, prob: np.ndarray,\n",
    "         mean:pd.DataFrame, Sigma: pd.DataFrame, itmax: int):\n",
    "    \"\"\"Main EM algorithm\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame: Input data, shape (n, p)\n",
    "        G (int): Number of classes\n",
    "        prob (np.ndarray): Mixing weights, shape (G,)\n",
    "        mean (pd.DataFrame): Distribution means for each class, shape (p, G)\n",
    "        Sigma (pd.DataFrame): Shared covariance matrix, shape (p, p)\n",
    "        itmax (int): Number of EM iterations to perform\n",
    "\n",
    "    Returns:\n",
    "        (np.ndarray, pd.DataFrame, pd.DataFrame, float): probability vector, means, covariance and\n",
    "                                                         log-likelihood\n",
    "    \"\"\"\n",
    "    for _ in range(itmax):\n",
    "        resp = Estep(data, G, prob, mean, Sigma)\n",
    "        prob, mean, Sigma = Mstep(data, G, resp, Sigma)\n",
    "        ll = loglik(data, G, prob, mean, Sigma)\n",
    "    return prob, mean, Sigma, ll   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing EM Algorithm for Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eruptions</th>\n",
       "      <th>waiting</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.600</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.800</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.333</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.283</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.533</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   eruptions  waiting\n",
       "1      3.600       79\n",
       "2      1.800       54\n",
       "3      3.333       74\n",
       "4      2.283       62\n",
       "5      4.533       85"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in data\n",
    "data = pd.read_csv('faithful.dat', header=0, sep='\\s+')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 1: G=2\n",
    "\n",
    "The two-class case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case G=2\n",
      "prob\n",
      "[0.04297883 0.95702117]\n",
      "\n",
      "mean\n",
      "                   0          1\n",
      "eruptions   3.495642   3.487430\n",
      "waiting    76.797892  70.632059\n",
      "\n",
      "Sigma\n",
      "           eruptions     waiting\n",
      "eruptions   1.297936   13.924336\n",
      "waiting    13.924336  182.580092\n",
      "\n",
      "loglik\n",
      "-1289.569\n",
      "\n"
     ]
    }
   ],
   "source": [
    "G = 2\n",
    "n = data.shape[0]\n",
    "p1 = 10 / n\n",
    "p2 = 1 - p1\n",
    "mu1 = data.iloc[:10, :].mean(axis=0)\n",
    "mu2 = data.iloc[10:, :].mean(axis=0)\n",
    "\n",
    "sigma = 1 / n * (\n",
    "           (data.iloc[:10, :].T - mu1.to_numpy().reshape(-1, 1)) @\n",
    "           (data[:10].T - mu1.to_numpy().reshape(-1, 1)).T + \\\n",
    "           (data.iloc[10:, :].T - mu2.to_numpy().reshape(-1, 1)) @\n",
    "           (data[10:].T - mu2.to_numpy().reshape(-1, 1)).T\n",
    "        )\n",
    "\n",
    "pi = np.array((p1, p2)) # Shape (G,)\n",
    "mu = pd.DataFrame({\"0\": mu1, \"1\": mu2}) # Shape (p, G)\n",
    "\n",
    "prob, mean, Sigma, ll = myEM(data, G, pi, mu, sigma, 20)\n",
    "print(\"Case G=2\")\n",
    "print(f\"prob\\n{prob}\\n\\nmean\\n{mean}\\n\\nSigma\\n{Sigma}\\n\\nloglik\\n{ll:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 2: G=3\n",
    "\n",
    "The three-class case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob\n",
      "[0.04363422 0.07718656 0.87917922]\n",
      "\n",
      "mean\n",
      "                   0          1          2\n",
      "eruptions   3.510069   2.816167   3.545641\n",
      "waiting    77.105638  63.357526  71.250848\n",
      "\n",
      "Sigma\n",
      "           eruptions     waiting\n",
      "eruptions   1.260158   13.511538\n",
      "waiting    13.511538  177.964191\n",
      "\n",
      "loglik\n",
      "-1289.351\n",
      "\n"
     ]
    }
   ],
   "source": [
    "G = 3\n",
    "p1 = 10 / n\n",
    "p2 = 20 / n\n",
    "p3 = 1 - p1 - p2\n",
    "mu1 = data.iloc[:10, :].mean(axis=0)\n",
    "mu2 = data.iloc[10:30, :].mean(axis=0)\n",
    "mu3 = data.iloc[30:, :].mean(axis=0)\n",
    "sigma = 1 / n * (\n",
    "           (data.iloc[:10].T - mu1.to_numpy().reshape(-1, 1)) @\n",
    "           (data.iloc[:10].T - mu1.to_numpy().reshape(-1, 1)).T + \\\n",
    "           (data.iloc[10:30].T - mu2.to_numpy().reshape(-1, 1)) @\n",
    "           (data.iloc[10:30].T - mu2.to_numpy().reshape(-1, 1)).T + \\\n",
    "           (data.iloc[30:].T - mu3.to_numpy().reshape(-1, 1)) @\n",
    "           (data.iloc[30:].T - mu3.to_numpy().reshape(-1, 1)).T\n",
    "        )\n",
    "\n",
    "pi = np.array((p1, p2, p3)) # Shape (G,)\n",
    "mu = pd.DataFrame({\"0\": mu1, \"1\": mu2, \"2\": mu3}) # Shape (p, G)\n",
    "\n",
    "prob, mean, Sigma, ll = myEM(data, G, pi, mu, sigma, 20)\n",
    "print(f\"prob\\n{prob}\\n\\nmean\\n{mean}\\n\\nSigma\\n{Sigma}\\n\\nloglik\\n{ll:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: HMM\n",
    "\n",
    "### Baum-Welch Algorithm\n",
    "\n",
    "**E-step**\n",
    "\n",
    "Forward probabilities, $\\alpha_t(i) = p_{\\theta}(x_1, \\ldots , x_t, Z_t = i)$, are updated recursively as follows:\n",
    "\n",
    "$$\\alpha_1(i) = w(i) B(i, x_1)$$\n",
    "\n",
    "$$\\alpha_{t+1}(i) = \\sum_j \\alpha_t(j) A(j,i) B(i, x_{t+1})$$\n",
    "\n",
    "Backward probabilities, $\\beta_t(i) = p_{\\theta}(x_{t+1}, \\ldots,  x_n \\mid Z_t = i)$ are also updated recursively:\n",
    "\n",
    "$$\\beta_n(i) = 1$$\n",
    "$$\\beta{t}(i) = \\sum_j A(i, j) B(j, x_{t+1}) \\beta_{t+1}(j)$$\n",
    "\n",
    "Finally, $\\alpha$ and $\\beta$ are used, along with the transition and emission matrices, to estimate the probability of transitioning between each pair of hidden states, given the observed data, $\\gamma_t(i, j) = p_{\\theta}(Z_t = i, Z_{t+1} = j \\mid x)$. It is calculated as follows.\n",
    "\n",
    "$$\\gamma_t(i,j) = \\alpha_t(i) A(i, j) B(j, x_{t + 1}) \\beta_{t+1}(j)$$\n",
    "\n",
    "\n",
    "**M-step**\n",
    "\n",
    "$w$ is not updated for this assignment.\n",
    "\n",
    "The updated transition matrix, $A^*$ is calculated by\n",
    "$$A^*(i, j) = \\frac{\\sum_{t=1}^{n-1}\\gamma_t(i,j)}{\\sum_{j'} \\sum_{t=1}^{n-1} \\gamma_t(i, j)}$$\n",
    "\n",
    "To update B, we first need to compute the marginal probability for $\\gamma$. There are two approaches to marginalizing $\\gamma$ over the hidden states, depending on the time value.\n",
    "\n",
    "For $t < n$, $\\gamma_t(i)$ we use the formula below.\n",
    "\n",
    "$$P(Z_t=i \\mid x) = \\sum_{j=1}^{m_z} P(Z_t=i, Z_{t+1} = j \\mid x) = \\sum_{j=1}^{m_z} \\gamma_t(i j)$$\n",
    "\n",
    "For $t = n$, $\\gamma_t(i)$ we use:\n",
    "\n",
    "$$P(Z_t=i \\mid x) = \\sum_{j=1}^{m_z} P(Z_{t-1}=j, Z_t = i \\mid x) = \\sum_{j=1}^{m_z} \\gamma_{t-1}(j, i)$$\n",
    "\n",
    "With the marginal probability of $\\gamma$, the updated emission matrix, $B^*$ can be calculated:\n",
    "\n",
    "$$B^*(i, l) = \\frac{\\sum_{t:x_t = l} \\gamma_t(i)} {\\sum_t \\gamma_t(i)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BW_onestep(data: np.ndarray, mx: np.ndarray, mz: np.ndarray,\n",
    "               w: np.ndarray, A: np.ndarray, B: np.ndarray):\n",
    "    \"\"\"Perform one iteration of the Baum-Welch algorithm, improving the estimates of\n",
    "       the transition probability and emission distribution matrices.\n",
    "\n",
    "    Args:\n",
    "        data (np.ndarray): Observations\n",
    "        mx (int): Count of distinct values X can take\n",
    "        mz (int): Count of distinct values Z can take\n",
    "        w (np.ndarray): An mz-by-1 probability vector representing the initial distribution for Z1.\n",
    "        A (np.ndarray): The mz-by-mz transition probability matrix that\n",
    "                        models the progression from Zt to Zt+1\n",
    "        B (np.ndarray): The mz-by-mx emission probability matrix,\n",
    "                        indicating how X is produced from Z\n",
    "\n",
    "    Returns:\n",
    "        (np.ndarray, np.ndarray): Updated A and B matrices\n",
    "    \"\"\"\n",
    "    n = data.shape[0]\n",
    "    \n",
    "    # E-step\n",
    "    # ==========================================================\n",
    "\n",
    "    # Forward algorithm\n",
    "    # Alpha is an mz-by-T forward probability matrix\n",
    "    alpha = np.empty((mz, n))\n",
    "    alpha[:, 0] = np.multiply(w, B[:, data[0]])\n",
    "    for t in range(n - 1):\n",
    "        alpha[:, t + 1] = (A.T @ alpha[:, t]) * B[:, data[t + 1]]\n",
    "    \n",
    "    # Backward algorithm\n",
    "    # Beta is an mz-by-T backwards probability matrix\n",
    "    beta = np.empty((mz, n))\n",
    "    beta[:, n - 1] = 1\n",
    "    for t in np.arange(n - 2, -1, step = -1):\n",
    "        beta[:, t] = A @ (B[:, data[t + 1]] * beta[:, t + 1])\n",
    "\n",
    "    # Gamma\n",
    "    gamma = np.empty((mz, mz, n - 1))\n",
    "    for t in range(n - 1):\n",
    "        for j in range(mz):\n",
    "            gamma[:, j, t] = alpha[:, t] * A[:, j] * B[j, data[t + 1]] * beta[j, t + 1]\n",
    "\n",
    "    # M-step\n",
    "    # ==========================================================\n",
    "\n",
    "    # Update A\n",
    "    A = gamma.sum(axis=2) # Sum over time\n",
    "    A /= A.sum(axis=1).reshape(-1, 1)\n",
    "\n",
    "    # Update B\n",
    "    # Marginalized gamma: mz-by-n\n",
    "    gamma_marginal = np.empty((mz, n))\n",
    "    gamma_marginal[:, :n - 1] = gamma.sum(axis=1)\n",
    "    gamma_marginal[:, n - 1] = gamma[:, :, n - 2].sum(axis=0)\n",
    "    for l in range(mx):\n",
    "        B[:, l] = gamma_marginal[:, data == l].sum(axis=1) / gamma_marginal.sum(axis=1)\n",
    "    return A, B\n",
    "\n",
    "def myBW(data: np.ndarray, mx: int, mz: int, w: np.ndarray,\n",
    "         A: np.ndarray, B: np.ndarray, itmax: int):\n",
    "    \"\"\"Perform the Baum-Welch algorithm for the Hidden Markov Model to estimate\n",
    "       the transition probability and emission distribution matrices.\n",
    "\n",
    "    Args:\n",
    "        data (np.ndarray): Observations\n",
    "        mx (int): Count of distinct values X can take\n",
    "        mz (int): Count of distinct values Z can take\n",
    "        w (np.ndarray): An mz-by-1 probability vector representing the initial distribution for Z1.\n",
    "        A (np.ndarray): The mz-by-mz transition probability matrix that\n",
    "                        models the progression from Zt to Zt+1\n",
    "        B (np.ndarray): The mz-by-mx emission probability matrix,\n",
    "                        indicating how X is produced from Z\n",
    "        itmax (int): Maximum number of EM step iterations to perform\n",
    "    \"\"\"\n",
    "    # Convert range of X values fron [1, 3] to  [0, 2] to facilitate indexing in Python\n",
    "    data = data - 1\n",
    "    for _ in range(itmax):\n",
    "        A, B = BW_onestep(data, mx, mz, w, A, B)\n",
    "    return A, B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Implementation of Baum-Welch algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: the 2-by-2 transition matrix\n",
      "\n",
      "[[0.49793938 0.50206062]\n",
      " [0.44883431 0.55116569]]\n",
      "\n",
      "B: the 2-by-3 emission matrix\n",
      "\n",
      "[[0.22159897 0.20266127 0.57573976]\n",
      " [0.34175148 0.17866665 0.47958186]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('coding4_part2_data.txt', header=None).to_numpy().flatten()\n",
    "\n",
    "# Establish possible observations and number of latent states\n",
    "mx = np.unique(data).shape[0] # Unique X values\n",
    "mz = 2 # Given in instructions\n",
    "\n",
    "# Initialize transition probability and emission distribution matrices\n",
    "w = np.array((0.5, 0.5))\n",
    "A = np.full((2, 2), 0.5)\n",
    "B = np.row_stack([np.array([1, 3, 5]) / 9,\n",
    "                  np.array([1, 2, 3]) / 6])\n",
    "\n",
    "# Perform Baum-Welch to find estimates of A and B\n",
    "A, B = myBW(data, mx, mz, w, A, B, 100)\n",
    "print(f\"A: the {mz}-by-{mz} transition matrix\\n\\n{A}\\n\\n\"\n",
    "      f\"B: the {mz}-by-{mx} emission matrix\\n\\n{B}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viterbi Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myViterbi(data: np.ndarray, mx: int, mz: int, w: np.ndarray,\n",
    "         A: np.ndarray, B: np.ndarray, itmax: int):\n",
    "    \"\"\"Perform the Viterbi Algorithm to output the most likely latent sequence considering \n",
    "        the data and the MLE of the parameters.\n",
    "\n",
    "    Args:\n",
    "        data (np.ndarray): Observations\n",
    "        mx (int): Count of distinct values X can take\n",
    "        mz (int): Count of distinct values Z can take\n",
    "        w (np.ndarray): An mz-by-1 probability vector representing the initial distribution for Z1.\n",
    "        A (np.ndarray): The mz-by-mz transition probability matrix that\n",
    "                        models the progression from Zt to Zt+1\n",
    "        B (np.ndarray): The mz-by-mx emission probability matrix,\n",
    "                        indicating how X is produced from Z\n",
    "        itmax (int): Maximum number of EM step iterations to perform\n",
    "    \"\"\"\n",
    "\n",
    "    # Perform Baum-Welch to find estimates of A and B\n",
    "    A, B = myBW(data, mx, mz, w, A, B, itmax)\n",
    "\n",
    "    # put all valus on log-scale\n",
    "    w = np.log(w)\n",
    "    A = np.log(A)\n",
    "    B = np.log(B)\n",
    "\n",
    "    # initialize additional parameters\n",
    "    n = data.shape[0]\n",
    "    delta = np.zeros((mz, n))\n",
    "    Z = np.zeros(n, dtype=int)\n",
    "\n",
    "    # subtract 1 from data, python is indexed by 0 as the start.  \n",
    "    data = data - 1\n",
    "\n",
    "    # set initial delta value\n",
    "    delta[:, 0] = w + B[:, data[0]]\n",
    "\n",
    "    # update for delta\n",
    "    for idx in range(n - 1):\n",
    "        delta[:, idx + 1] = np.max(A + delta[:, idx].reshape(-1,1), axis=0) + B[:, data[idx + 1]]\n",
    "    # print(delta.T)\n",
    "    \n",
    "    # find optimal Z value. \n",
    "    Z[n-1] = np.argmax(delta[:, n-1])\n",
    "    for idx in range(n-1, 0, -1):\n",
    "        Z[idx - 1] = np.argmax(delta[:, idx-1] + A[:, Z[idx]])\n",
    "\n",
    "    # add one at the end to match output\n",
    "    return Z + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Implementation of the Viterbi Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== Z Valid ========================\n",
      "\n",
      "[1 1 1 1 1 1 1 2 1 1 1 1 1 2 2 1 1 1 1 1 1 1 2 2 2 2 2 1 1 1 1 1 1 1 2 1 1\n",
      " 1 1 1 1 1 1 2 2 1 1 1 1 1 1 2 2 2 1 1 1 1 2 2 2 2 1 1 1 1 1 1 1 1 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1\n",
      " 1 1 1 2 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2\n",
      " 2 2 2 1 1 1 2 2 2 2 2 2 1 1 1 1 1 2 2 2 2 2 2 2 2 2 1 1 1 2 2 2 1 1 1 1 1\n",
      " 1 1 1 2 2 2 2 2 1 1 1 1 1 1 1] \n",
      "\n",
      "================== Z Calculated ===================\n",
      "\n",
      "[1 1 1 1 1 1 1 2 1 1 1 1 1 2 2 1 1 1 1 1 1 1 2 2 2 2 2 1 1 1 1 1 1 1 2 1 1\n",
      " 1 1 1 1 1 1 2 2 1 1 1 1 1 1 2 2 2 1 1 1 1 2 2 2 2 1 1 1 1 1 1 1 1 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1\n",
      " 1 1 1 2 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2\n",
      " 2 2 2 1 1 1 2 2 2 2 2 2 1 1 1 1 1 2 2 2 2 2 2 2 2 2 1 1 1 2 2 2 1 1 1 1 1\n",
      " 1 1 1 2 2 2 2 2 1 1 1 1 1 1 1] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('coding4_part2_data.txt', header=None).to_numpy().flatten()\n",
    "\n",
    "# Establish possible observations and number of latent states\n",
    "mx = np.unique(data).shape[0] # Unique X values\n",
    "mz = 2 # Given in instructions\n",
    "\n",
    "# Initialize transition probability and emission distribution matrices\n",
    "w = np.array((0.5, 0.5))\n",
    "A = np.full((2, 2), 0.5)\n",
    "B = np.row_stack([np.array([1, 3, 5]) / 9,\n",
    "                  np.array([1, 2, 3]) / 6])\n",
    "\n",
    "# Load in valid Z values for comparison\n",
    "Z_valid = []\n",
    "with open('Coding4_part2_Z.txt', 'r') as f:\n",
    "    Z_valid = np.array(f.read().strip().split(' ')).astype(int)\n",
    "\n",
    "# Run Viterbi algorithm\n",
    "Z = myViterbi(data, mx, mz, w, A, B, 100)\n",
    "\n",
    "# Output results\n",
    "print('================== Z Valid ========================\\n')\n",
    "print(Z_valid, '\\n')\n",
    "print('================== Z Calculated ===================\\n')\n",
    "print(Z, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We verify that our output is identical to the expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Z Valid == Z Calc: True\n"
     ]
    }
   ],
   "source": [
    "print('\\nZ Valid == Z Calc:', np.array_equal(Z, Z_valid))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
