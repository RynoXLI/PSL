{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Vocabulary Construction\n",
    "\n",
    "CS 598 Practical Statistical Learning\n",
    "\n",
    "2023-12-03\n",
    "\n",
    "UIUC Fall 2023\n",
    "\n",
    "**Authors**\n",
    "* Ryan Fogle\n",
    "    - rsfogle2@illinois.edu\n",
    "    - UIN: 652628818\n",
    "* Sean Enright\n",
    "    - seanre2@illinois.edu\n",
    "    - UIN: 661791377\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook we describe the process we followed to generate a suitably small, but relevant vocabulary for the task of sentiment analysis and classification with the [IMDB movie review](https://www.kaggle.com/c/word2vec-nlp-tutorial/data) dataset.\n",
    "\n",
    "Our goal was to identify a set of words or phrases that were fewer than 1000 in number, but capable of predicting movie sentiment from the review text with high accuracy, using AUROC as the scoring metric.\n",
    "\n",
    "## Data Retrieval\n",
    "Here we retrieve and format the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The IMDB movie review data is split into five folds for cross-validation. The following functions are used to read the training and test data for a given split.\n",
    "\n",
    "The dataset contains columns for `id`, `score`, `sentiment`, and `review`. We are interested in using the `review` text as our input and classifiying  `sentiment`, which uses a value of $0$ for a poor review and $1$ for a good one. The `id` column indexes each observation.\n",
    "\n",
    "The `review` text is also stripped of character sequences used for HTML tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes_dict = {\"review\": \"string\",\n",
    "               \"sentiment\": \"Int32\"}\n",
    "\n",
    "def get_data(base_path=Path.cwd()):\n",
    "    \"\"\"Retrieve the training and test data, formatted as DataFrames\n",
    "\n",
    "    Args:\n",
    "        base_path (optional): Path of folder with training and test data.\n",
    "                              Defaults to Path.cwd().\n",
    "\n",
    "    Returns:\n",
    "        (train_x, train_y, test_x): Training and test dataframes\n",
    "    \"\"\"\n",
    "    path_train  = base_path / \"train.tsv\"\n",
    "    path_test   = base_path / \"test.tsv\"\n",
    "    \n",
    "    train = pd.read_csv(path_train, sep=\"\\t\", header=0, dtype=dtypes_dict)\n",
    "    train_x = train[\"review\"].str.replace(\"&lt;.*?&gt;\", \" \", regex=True)\n",
    "    train_y = train[\"sentiment\"]\n",
    "\n",
    "    test = pd.read_csv(path_test, sep=\"\\t\", header=0,\n",
    "                       dtype=dtypes_dict, index_col=\"id\")\n",
    "    test_x = test[\"review\"].str.replace(\"&lt;.*?&gt;\", \" \", regex=True)\n",
    "    return train_x, train_y, test_x\n",
    "\n",
    "def get_fold_data(fold):\n",
    "    \"\"\"Retrieve the training and test data for a given fold\n",
    "\n",
    "    Args:\n",
    "        fold (int): Fold number for training andtest data\n",
    "\n",
    "    Returns:\n",
    "        (train_x, train_y, test_x, test_y): Training and test dataframes\n",
    "    \"\"\"\n",
    "    fold_path = Path.cwd() / \"proj3_data\" / f\"split_{fold}\"\n",
    "    train_x, train_y, test_x = get_data(base_path=fold_path)\n",
    "    path_test_y = fold_path / \"test_y.tsv\"\n",
    "\n",
    "    test_y = pd.read_csv(path_test_y, sep=\"\\t\", header=0, dtype=dtypes_dict)[\"sentiment\"]\n",
    "    return train_x, train_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a more relevant vocabulary, the training and test data are joined for a split. This expands the possibly vocabulary beyond what might be found in a given split, and benefits from the greater sample size.\n",
    "\n",
    "We select split #2, as it had proven in testing to prove the greatest challenge to our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 2\n",
    "train_x, train_y, test_x, test_y = get_fold_data(fold)\n",
    "full_x = pd.concat((train_x, test_x), axis=0)\n",
    "full_y = pd.concat((train_y, test_y), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing and Prediction Pipeline\n",
    "\n",
    "An overview of our preprocessing and prediction pipeline:\n",
    "1) Tokenize review text into a matrix of n-grams\n",
    "2) Convert n-gram count into TF-IDF\n",
    "3) Perform two-sample t-tests between n-grams to select a subset that is more relevant to positive or negative reviews.\n",
    "4) Use cross-validation to find the Lasso regularization weight for logistic regression that further reduces the subset of n-grams below a predefined count\n",
    "5) Refit the data with the Lasso regularization weight found above and retrieve the subset of selected n-grams\n",
    "\n",
    "We use `sklearn`'s Pipeline API to streamline the preprocessing of the review data and feature selection. Below we will define each step of the pipeline and then show the complete process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "To convert the input into a matrix of n-grams, the `sklearn.feature_extraction.text.CountVectorizer` class is used. The following configuration is used for this class:\n",
    "* Words are converted to lowercase.\n",
    "* n-grams between 1 and 4 in length are generated from the review text.\n",
    "* Document frequency (\"df\") is restricted to the range of 0.001 to 0.5.\n",
    "* The tokenizer pattern selects words consisting of word characters followed by a pipe or apostrophe. These are then used to construct n-grams.\n",
    "\n",
    "To ignore commonly used words unlikely to carry much information, the below list of stop words is used. We add the word \"br\" to the set of words suggested by the instructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\n",
    "    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\",\n",
    "    \"yours\", \"their\", \"they\", \"his\", \"her\", \"she\", \"he\", \"a\", \"an\", \"and\", \"is\",\n",
    "    \"was\", \"are\", \"were\", \"him\", \"himself\", \"has\", \"have\", \"it\", \"its\", \"the\",\n",
    "    \"us\", \"br\"\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    preprocessor=lambda x: x.lower(), # Convert to lowercase\n",
    "    stop_words=stop_words,            # Remove stop words\n",
    "    ngram_range=(1, 4),               # Use 1- to 4-grams\n",
    "    min_df=0.001,                     # Minimum term frequency\n",
    "    max_df=0.5,                       # Maximum document frequency\n",
    "    token_pattern=r\"\\b[\\w+\\|']+\\b\"    # Use word tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Conversion\n",
    "The n-gram counts are converted into term-frequency times inverse document-frequency (TF-IDF) to weigh them within the context of the document (review), and amongst other documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer(use_idf=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-test Subset Selection\n",
    "To accomplish the first reduction vocabulary size, we apply a two-sample t-test to each n-gram, comparing the difference between the class of negative reviews and the class of positive reviews, and filter by the magnitude of the t-test statistics. n-grams with high magnitude are more likely to be relevant in deciding whether the n-gram is associated with a positive and negative review, and therefore are better suited for classification and should be selected.\n",
    "\n",
    "For random variables $X$ and $Y$, each drawn from a different class corresponding to either positive or negative reviews, the t-test statistic is\n",
    "$$\n",
    "t = \n",
    "\\frac{\\bar{X} - \\bar{Y}}\n",
    "     {\\sqrt{\\frac{s_X^2}{m} + \\frac{s_Y^2}{n}}}\n",
    "$$\n",
    "where $m$ and $n$ are the number of observations in classes $X$ and $Y$, respectively, and $s_X^2$ and $s_Y^2$ are the sample variances.\n",
    "\n",
    "The built-in `numpy` variance function proved to have a high memory demand, so the following variance identity was used to perform the calculation more efficiently:\n",
    "$$\n",
    "\\text{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2\n",
    "$$\n",
    "\n",
    "The magnitude of the t-statistic was then used to select a predefined number of n-grams.\n",
    "\n",
    "In order to incorporate this transformation with the Pipeline, we create a class that inherits from `BaseEstimator` and `TransformerMixin` and define its fitting and transformation methods.\n",
    "\n",
    "In this example, we select the 2000 n-grams with the greatest t-statistic magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T_TestTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vocab_size=2000):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.subset_inds = []\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        # Use two-sample t-test to determine n-grams more likely\n",
    "        # to be associated with positive or negative reviews\n",
    "        mask = y.values.to_numpy() == 1\n",
    "        pos_x = x[mask]\n",
    "        neg_x = x[~mask]\n",
    "        \n",
    "        m = pos_x.shape[0]\n",
    "        n = neg_x.shape[0]\n",
    "        mean_pos = pos_x.mean(axis=0)\n",
    "        mean_neg = neg_x.mean(axis=0)\n",
    "        # Var(X) = E[X^2] - (E[X])^2\n",
    "        var_pos = pos_x.power(2).mean(axis=0) - np.power(pos_x.mean(axis=0), 2)\n",
    "        var_neg = neg_x.power(2).mean(axis=0) - np.power(neg_x.mean(axis=0), 2)\n",
    "        t_stat = (mean_pos - mean_neg) / np.sqrt(var_pos / m + var_neg / n)\n",
    "        self.subset_inds = np.abs(np.ravel(t_stat)).argsort()[-self.vocab_size:]\n",
    "        return self\n",
    "\n",
    "    def transform(self, x, y=None):\n",
    "        # Select columns corresponing to n-grams likely to be relevant\n",
    "        return x[:, self.subset_inds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with Lasso Regularization\n",
    "\n",
    "We use logistic regression with Lasso regularization to select an even smaller subset of relevant features. To find the optimal regularization weight, we perform cross-validated logistic regression via `LogisticRegressionCV` and search a range of logarithmically scaled weights, scored by AUROC.\n",
    "\n",
    "Cross-validation is performed by the default parameter, which is [stratified k-folds](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html) with $k=5$ splits. We use the \"saga\" solver, since this dataset is on the large side.\n",
    "\n",
    "To ensure repeatability, a random seed is set, and to ensure convergence, we allow up to 1000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs = np.logspace(-2, -0.4, 20)\n",
    "logistic_regression = LogisticRegressionCV(\n",
    "    n_jobs=-1,\n",
    "    Cs=Cs,\n",
    "    solver=\"saga\",\n",
    "    penalty=\"l1\",\n",
    "    scoring=\"roc_auc\",\n",
    "    max_iter=1000,\n",
    "    random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Execution\n",
    "\n",
    "The above transformation and classification steps are defined in a `Pipeline`, which is then used to fit the combined training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_vocab = Pipeline([\n",
    "    (\"vectorizer\", vectorizer),\n",
    "    (\"t-score\", T_TestTransformer()),\n",
    "    (\"tfidf\", tfidf_transformer),\n",
    "    (\"logreg\", logistic_regression),\n",
    "])\n",
    "\n",
    "pipeline_vocab.fit(full_x, full_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting the Lasso Regularization Parameter\n",
    "The optimal regularization weight is found by first filtering by weights that produced a final vocabulary size smaller than a predefined size, and then selecting the weight within this subset that produces the highest AUROC.\n",
    "\n",
    "We have specified a maximum final vocabulary size of 950 n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab = 950\n",
    "vocab_size = np.empty(Cs.shape)\n",
    "scores = pipeline_vocab[\"logreg\"].scores_[1].mean(axis=0)\n",
    "for i in range(len(Cs)):\n",
    "    vocab_size[i] = (pipeline_vocab[\"logreg\"].coefs_paths_[1]\n",
    "                     .mean(axis=0)[i, :] != 0).sum() - 1\n",
    "mask = vocab_size < max_vocab\n",
    "best_c = Cs[mask][np.argmax(scores[mask])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This regularization weight is used to define a new classifier, which replaces the cross-validated classifier in the pipeline. Then we refit the data with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_vocab.steps.pop(3)\n",
    "pipeline_vocab.steps.append(\n",
    "    [\"logreg\", LogisticRegression(n_jobs=-1,\n",
    "                                  C=best_c,\n",
    "                                  solver=\"saga\",\n",
    "                                  penalty=\"l1\",\n",
    "                                  max_iter=1000)])\n",
    "\n",
    "pipeline_vocab.fit(full_x, full_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Vocabulary Selection\n",
    "We then select all of the non-zero logistic regression coefficients to identify the final subset of relevant n-grams. The column indices for the t-test and regression are referenced against the `CountVectorizer` columns to identify a list of n-gram strings for our vocabulary.\n",
    "\n",
    "With the provided configuration, this gives us a vocabulary of 874 n-grams.\n",
    "\n",
    "Finally, this vocabulary is written to file with each line corresponding to a space-delimited n-gram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select vocabulary\n",
    "t_score_inds = pipeline_vocab[\"t-score\"].subset_inds\n",
    "vocab_t = pipeline_vocab[\"vectorizer\"].get_feature_names_out()[t_score_inds]\n",
    "lasso_inds = (pipeline_vocab[\"logreg\"].coef_ != 0).reshape(-1)\n",
    "vocab = vocab_t[lasso_inds]\n",
    "print(f\"\\nVocab size  (t-test selection): {len(vocab_t)}\")\n",
    "print(f\"  Vocab size (logistic w/ lasso): {len(vocab)} words\\n\")\n",
    "\n",
    "# Write vocab to file\n",
    "pd.DataFrame(vocab).to_csv(\"myvocab.txt\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Time (s):', (datetime.now() - start_time).total_seconds())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
