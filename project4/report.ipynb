{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4: Movie Recommender System\n",
    "\n",
    "CS 598 Practical Statistical Learning\n",
    "\n",
    "2023-12-10\n",
    "\n",
    "UIUC Fall 2023\n",
    "\n",
    "**Authors**\n",
    "* Ryan Fogle\n",
    "    - rsfogle2@illinois.edu\n",
    "    - UIN: 652628818\n",
    "* Sean Enright\n",
    "    - seanre2@illinois.edu\n",
    "    - UIN: 661791377\n",
    "\n",
    "**Contributions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_dir = Path('.') / 'ml-1m' / 'ml-1m' \n",
    "ratings = pd.read_csv(movie_dir / 'ratings.dat', sep='::', engine = 'python', header=None)\n",
    "ratings.columns = ['UserID', 'MovieID', 'Rating', 'Timestamp']\n",
    "movies = pd.read_csv(movie_dir / 'movies.dat', sep='::', engine = 'python',\n",
    "                     encoding=\"ISO-8859-1\", header = None)\n",
    "movies.columns = ['MovieID', 'Title', 'Genres']\n",
    "users = pd.read_csv(movie_dir / 'users.dat', sep='::', engine='python', header=None)\n",
    "users.columns = ['UserID', 'Gender', 'Age', 'Occupation', 'Zipcode']\n",
    "\n",
    "# Create new entry for each genre in a movie, join movie and ratings together. \n",
    "movies['Genres'] = movies['Genres'].str.split('|')\n",
    "df = movies.merge(ratings, left_on=\"MovieID\", right_on=\"MovieID\")\n",
    "df = df.explode('Genres')\n",
    "df.rename(columns={'Genres': 'Genre'}, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a MovieID -> Title map for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mov_title_map = dict(zip(movies['MovieID'], movies['Title']))\n",
    "list(mov_title_map.items())[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System I: Recommendation Based on Genres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General idea: Use Bayseian probabilty to calcuate new ratings based upon additional prior assumptions, and then rank movies by this new rating to select the top 5 per genre.\n",
    "\n",
    "This algorthm is based upon this stack overflow post:\n",
    "https://stackoverflow.com/questions/2495509/how-to-balance-number-of-ratings-versus-the-ratings-themselves\n",
    "\n",
    "$$\n",
    "\\tilde{R} = \\frac{\\bar{w} \\bar{r} + \\sum_{i=1}^{n}{r_i}}{\\bar{w} + n}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\tilde{R}$ is the new rating\n",
    "- $\\bar{w}$ is the predefined number of ratings (weight) to include in our prior assumption\n",
    "- $\\bar{r}$ is the predefined average rating to include in our prior assumption\n",
    "- $n$ is the number of ratings\n",
    "- $r_i$ is the rating for a given entry.\n",
    "\n",
    "**Interpretation**:\n",
    "\n",
    "Consider $\\bar{w}$ to be the average number of ratings for a given genre, and $\\bar{r}$ to be the number of times to consider that rating for a given genre. We first assume the rating of a movie to be defined as $\\frac{\\bar{w} \\bar{r}}{\\bar{w}}$ when $n=0$, then slightly update the estimate for each new given rating. \n",
    "\n",
    "In our implementation we defined $\\bar{w}$ to be the median genre rating from the average movie ratings in that genre. We also define $\\bar{r}$ to be the genre's 25th percentile count (of ratings per movie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by Genre and Movie, this will be used to find median ratings and percentile counts for our prior\n",
    "gb = df.groupby(['Genre', 'MovieID'])\n",
    "\n",
    "# Median Genre ratings of Average Movie ratings\n",
    "median_ratings = gb['Rating'].mean().reset_index().groupby('Genre')['Rating'].median().reset_index()\n",
    "median_ratings = dict(zip(median_ratings['Genre'], median_ratings['Rating']))\n",
    "median_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab 25th Percentile of count by genre\n",
    "quantile_count = gb['Timestamp'].count().reset_index().groupby('Genre')['Timestamp'].quantile(0.25).reset_index()\n",
    "quantile_count.columns = ['Genre', 'Count']\n",
    "quantile_count = dict(zip(quantile_count['Genre'], quantile_count['Count']))\n",
    "quantile_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_ratings = []\n",
    "for (genre, movie_id), movie in gb:\n",
    "    n = movie.shape[0]\n",
    "    w = quantile_count[genre]\n",
    "    r = median_ratings[genre] \n",
    "    weighted_rating = (r * w + movie['Rating'].sum()) / (w + n)\n",
    "    weighted_ratings.append((genre, movie_id, mov_title_map[movie_id], weighted_rating, movie['Rating'].sum() / n, n))\n",
    "\n",
    "w_df = pd.DataFrame(weighted_ratings, columns=['Genre', 'MovieID', 'Title', 'WeightedRating', 'AverageRating', '# of Ratings'])\n",
    "w_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort ratings by WeightedRating, group by genre and grab the first five occurrences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sysI_recs = w_df.sort_values('WeightedRating', ascending=False).groupby('Genre').head(n=10).sort_values(['Genre', 'WeightedRating'], ascending=[True, False])\n",
    "sysI_recs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output System I recommends for the dashboard to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sysI_recs.to_csv('sysI_recs.csv', index=False)\n",
    "w_df.to_csv('sysI_recs_full.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System II: Recommendation Based on IBCF\n",
    "\n",
    "### Overview\n",
    "\n",
    "The recommendation system we have implemented follows these main steps:\n",
    "1) Collect the user and rating data for all considered movies.\n",
    "2) Calculate the centered cosine similarity between all pairs of movies (items) for the provided user rating data.\n",
    "3) Use Item-based Collaborative Filtering to predict the ratings of unrated movies.\n",
    "4) Suggest the movies with the highest predicted ratings.\n",
    "\n",
    "### Similarity Matrix Construction\n",
    "\n",
    "To construct the similarity matrix, we require user ratings for various items. The input rating matrix is $R_{a \\times i}$, where $a$ is the number of users who have reviewed one or more movie, and $i$ is the number of movies.\n",
    "\n",
    "In the case of our dataset, there are 6040 users and 3706 movies, so $R$ is of shape $6040 \\times 3706$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_mov_df = pd.read_csv('Rmat.csv')\n",
    "user_mov_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization of Ratings Matrix\n",
    "We normalize the rating matrix by subtracting the row means from each row, ignoring `NA` entries. This addresses the variation in each user's average rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_mov_df_norm = user_mov_df.sub(user_mov_df.mean(axis=1, skipna=True), axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity\n",
    "\n",
    "We seek to compute the similarity between movies (items). We select centered cosine similarity as our measure of similarity. Having normalized our ratings matrix by each user's average rating, the next step is computation of similarity.\n",
    "\n",
    "Cosine similarity is defined generally as\n",
    "\n",
    "$$\n",
    "\\frac{(u - \\bar{u})^T (v - \\bar{v})}\n",
    "     {\\lVert u - \\bar{u} \\rVert \\cdot \\lVert v - \\bar{v} \\rVert}\n",
    "$$\n",
    "where $u$ and $v$ are vectors\n",
    "\n",
    "In our case, these vectors represent user ratings for a given movie. The above similarity value will range from $[-1, 1]$, but we prefer a range of $[0, 1]$, so we perform a transformation on the similarity accordingly. For each pair of movies $i$ and $j$, the similarity value $S_{ij}$ is\n",
    "\n",
    "$$\n",
    "S_{ij} =\n",
    "\\frac{1}{2} +\n",
    "\\frac{1}{2}\n",
    "     \\frac{\\sum_{l \\in \\mathcal{I}_{ij}} R_{li} R_{lj}}\n",
    "          {\\sqrt{\\sum_{l \\in \\mathcal{I}_{ij}} R_{li}^2} \\,\n",
    "           \\sqrt{\\sum_{l \\in \\mathcal{I}_{ij}} R_{lj}^2}}\n",
    "$$\n",
    "\n",
    "where $\\mathcal{I}_{ij}$ is the set of users who have both reviewed movies $i$ and $j$, and $R$ is the ratings matrix defined above.\n",
    "\n",
    "Additionally, we excluding similarity values for any movies with fewer than three ratings, i.e., items with cardinality < 3.\n",
    "\n",
    "Since the resulting similarity matrix $S$ is symmetric, we only compute the upper half, and then fill in the lower half by transposing it.\n",
    "\n",
    "Our implementation of the centered cosine similarity between items follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def cosine_similarity(x, min_cardinality=3):    \n",
    "    # Prepare symmetric result matrix\n",
    "    s = np.empty((x.shape[1], x.shape[1]))\n",
    "    s[:] = np.nan\n",
    "\n",
    "    # Calculate similarity for upper triangular matrix\n",
    "    for i in tqdm(range(0, x.shape[1] - 1)):\n",
    "        i_valid = ~np.isnan(x[:, i])\n",
    "        for j in range(i + 1, x.shape[1]):\n",
    "            j_valid = ~np.isnan(x[:, j])\n",
    "            row_mask = np.logical_and(i_valid, j_valid)\n",
    "            if row_mask.sum() >= min_cardinality:\n",
    "                r_li = x[row_mask, i]\n",
    "                r_lj = x[row_mask, j]\n",
    "                s[i, j] = (np.dot(r_li, r_lj)\n",
    "                           / (np.sqrt(np.power(r_li, 2).sum()) \n",
    "                              * np.sqrt(np.power(r_lj, 2).sum())))\n",
    "    s = 0.5 + s / 2\n",
    "\n",
    "    # Transpose upper triangular matrix to form lower\n",
    "    lower_idx = np.tril_indices(x.shape[1])\n",
    "    s[lower_idx] = s.T[lower_idx]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply this function to our centered ratings matrix, producing a symmetric similarity matrix $S_{i \\times i}$.\n",
    "\n",
    "We extract and re-wrap the column indices to retain the movie IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_cardinality = 3\n",
    "\n",
    "s = cosine_similarity(user_mov_df_norm.to_numpy(),\n",
    "                      min_cardinality=min_cardinality)\n",
    "s = pd.DataFrame(data=s,\n",
    "                 index=user_mov_df_norm.columns,\n",
    "                 columns=user_mov_df_norm.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation of Similarity Matrix Before Filtering\n",
    "\n",
    "In order to validate our similarity matrix and our implementation of centered cosine similarity, we show the pairwise similarity values from the $S$ matrix for the following specified movies:\n",
    "\n",
    "```m1, m10, m100, m1510, m260, m3212```\n",
    "\n",
    "We are validating our results against the values in [Campuswire post #861](https://campuswire.com/c/G06C55090/feed/861)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.precision\", 7)\n",
    "specified_movies = [\"m1\", \"m10\", \"m100\", \"m1510\", \"m260\", \"m3212\"]\n",
    "s.loc[specified_movies, specified_movies]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering by Most Similar Movies\n",
    "\n",
    "Next, for each movie, we determine the 30 most similar movies and set all other movies to NA. This allows for a more compact $S$ matrix. For movies that have fewer than 30 similar movies, all available similar movies (i.e., non-`NA`) are kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_similar = 30\n",
    "\n",
    "for i in range(s.shape[0]):\n",
    "    row = s.iloc[i, :]\n",
    "    num_selected = min([(~np.isnan(row)).sum(), max_similar, len(row)])\n",
    "    # Find max allowed similarity with NaN vals\n",
    "    max_sim = np.roll(np.sort(row)[::-1],\n",
    "                      -np.count_nonzero(np.isnan(row)))[num_selected - 1]\n",
    "    na_mask = row < max_sim\n",
    "    s.iloc[i, na_mask] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This filtered similarity matrix is written to file as `similarity.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.to_csv(\"similarity.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item-based Collaborative Filtering\n",
    "\n",
    "In Item-based Collaborative Filtering (IBCF), given a single set of ratings for a user or a hypothetic user, we seek to predict the ratings of unrated movies based on the ratings given by similar users.\n",
    "\n",
    "#### Implementation of ICBF\n",
    "\n",
    "**ICBF Calculation**\n",
    "\n",
    " For all non-rated movies the predicted rating. It is the inner product of all ratings seen in both the input rating vector and the similarity vector for that movie, normalized by the sum of similarity scores for these rated movies. It is computed as follows:\n",
    "\n",
    " $$\n",
    " \\frac\n",
    "    {1}\n",
    "    {\\sum_{i \\in S(l)} S_{li} \\textbf{1}_{w_i \\neq NA}}\n",
    " \\sum_{i \\in S(l)} S_{li} w_i\n",
    " $$\n",
    "\n",
    "where $w_i$ is a vector of movie ratings for a single user.\n",
    "\n",
    "Movies with no overlap between the input rating vector and the similarity vector for that movie are given an `NA` rating.\n",
    "\n",
    "**Results, Ordering and Special Cases**\n",
    "\n",
    "Having computed the predicted ratings for all movies, we return the `MovieID`s for the 10 movies with the highest predicted rating.\n",
    "\n",
    "In the case of tie breaks, movies are recommended by rating in descending order. We use the `WeightedRatings` column from the System I implementation for our definition of highest-rated movies. If further ties occur, the movie with the lowest `MovieID` is taken.\n",
    "\n",
    "If fewer than 10 recommendations are calculated, we fill the missing recommendations with the highest-rated movies in the user's most watched genres. In the case of a tie in most-watched genre, the genre with the lowest name in lexicographic order is taken.\n",
    "\n",
    "Both cases ensure that the recommended movies are unrated by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myIBCF(s, newuser, mov_rate_genre, genre_top_recs, num_recs=10):\n",
    "\n",
    "    recs = newuser.copy(deep=True).rename(\"PredictedRating\")\n",
    "    recs.iloc[:] = np.nan\n",
    "    \n",
    "    i_in_w = ~np.isnan(newuser)\n",
    "    # Compute predicted rating for all non-rated movies\n",
    "    for l in np.arange(newuser.shape[0])[np.isnan(newuser)]:\n",
    "        s_li = s.iloc[l, :]\n",
    "        i_in_sl = ~np.isnan(s_li)\n",
    "        col_mask = np.logical_and(i_in_sl, i_in_w)\n",
    "        if s_li[col_mask].sum() == 0:\n",
    "            continue\n",
    "        recs.iloc[l] = (\n",
    "            1 / (s_li[col_mask].sum())\n",
    "            * np.dot(s_li[col_mask], newuser[col_mask])\n",
    "        )\n",
    "    recs = recs[~np.isnan(recs)] \n",
    "    \n",
    "    movie_recs = mov_rate_genre.join(recs, how=\"inner\")\n",
    "    movie_recs.sort_values(by=[\"PredictedRating\", \"WeightedRating\"],\n",
    "                           axis=0, ascending=False, inplace=True)\n",
    "    #print(f\"# ratings: {np.count_nonzero(~np.isnan(newuser))}\")\n",
    "    #print(f\"   # recs: {recs.shape[0]}\")\n",
    "    if movie_recs.shape[0] >= num_recs:\n",
    "        movie_recs = movie_recs.iloc[:num_recs, :]\n",
    "        #print(movie_recs)\n",
    "        rec_movie_ids = movie_recs.index.tolist()\n",
    "    else:\n",
    "        # Begin with all available recommendations\n",
    "        rec_movie_ids = movie_recs.index.tolist()\n",
    "        \n",
    "        # Add remainding recommendations based on most rated genre\n",
    "        addl_recs = num_recs - movie_recs.shape[0]\n",
    "        \n",
    "        # Identify most-rated genre\n",
    "        rated_genres = mov_rate_genre[\"Genres\"][~np.isnan(newuser)]\n",
    "        genre_tup = np.unique(np.concatenate(rated_genres.values),\n",
    "                              return_counts=True)\n",
    "        most_watched_genre = genre_tup[0][np.argsort(genre_tup[1])[-1]]\n",
    "        \n",
    "        # Identify highest-rated movies in this genre\n",
    "        genre_recs = genre_top_recs[\n",
    "            genre_top_recs[\"Genre\"] == most_watched_genre]\n",
    "        \n",
    "        # Check that top movies in genre are unrated\n",
    "        genre_recs.loc[:, \"MovieID\"] = (\n",
    "            \"m\" + genre_recs.loc[:, \"MovieID\"].astype(str))\n",
    "        unwatched = [m not in newuser[i_in_w].index.tolist() \n",
    "                        for m in genre_recs[\"MovieID\"].tolist()]\n",
    "        genre_recs = genre_recs[unwatched][\"MovieID\"][:addl_recs].tolist()\n",
    "        \n",
    "        rec_movie_ids += genre_recs\n",
    "\n",
    "    return rec_movie_ids\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our implementation requires weighted rating data for each movie, as well the genre(s) of each movie. This is computed in advance and stored in `movie_ratings_genre.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link each MovieID to its rating\n",
    "mov_rate_genre = w_df[[\"MovieID\",\n",
    "                      \"WeightedRating\"]].groupby(\"MovieID\").mean()\n",
    "# Link each MovieID to its genre(s)\n",
    "mov_rate_genre[\"Genres\"] = (w_df[[\"MovieID\", \"Genre\"]]\n",
    "                                .groupby(\"MovieID\")[\"Genre\"]\n",
    "                                .apply(list))\n",
    "mov_rate_genre.index = \"m\" + mov_rate_genre.index.astype(str)\n",
    "mov_rate_genre.sort_values(by=\"MovieID\", inplace=True)\n",
    "\n",
    "mov_rate_genre.to_csv(\"movie_ratings_genre.csv\")\n",
    "\n",
    "# Read from file to simulate app\n",
    "mov_rate_genre = pd.read_csv(\"movie_ratings_genre.csv\", index_col=0,\n",
    "                             converters={\"Genres\": pd.eval})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation of `myIBCF`\n",
    "\n",
    "To validate our implementation of `myIBCF`, we show the top 10 recommendations for:\n",
    "* User \"u1181\" from rating matrix $R$\n",
    "* User \"u1351\" from rating matrix $R$\n",
    "* A hypothetical user who rates movie “m1613” with 5 and movie “m1755” with 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothetical_user = user_mov_df.iloc[0, :].copy(deep=True)\n",
    "hypothetical_user.iloc[:] = np.nan\n",
    "hypothetical_user.loc[[\"m1613\", \"m1755\"]] = [5, 4]\n",
    "\n",
    "test_users = [\n",
    "    (\"User u1181\", user_mov_df.loc[\"u1181\", :]),\n",
    "    (\"User u1351\", user_mov_df.loc[\"u1351\", :]),\n",
    "    (\"Hypothetical user\", hypothetical_user)\n",
    "]\n",
    "\n",
    "for username, w in test_users:\n",
    "    print(f\"\\n{username}\\n--{len(username)*'-'}\\n\")\n",
    "    print(myIBCF(s, w, mov_rate_genre, sysI_recs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test for edge-case of less than 10 recommendations given by IBCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothetical_user = user_mov_df.iloc[0, :].copy(deep=True)\n",
    "hypothetical_user.iloc[:] = np.nan\n",
    "hypothetical_user.loc[[\"m6\"]] = [5]\n",
    "\n",
    "test_users = [\n",
    "    (\"Hypothetical user\", hypothetical_user)\n",
    "]\n",
    "\n",
    "for username, w in test_users:\n",
    "    print(f\"\\n{username}\\n--{len(username)*'-'}\\n\")\n",
    "    print(myIBCF(s, w, mov_rate_genre, sysI_recs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Movie Recommendations\n",
    "\n",
    "In our application, in order to gauge the user's movie preferences, we provide an initial set of movies and ask for ratings to be input. This set of initial movies consists of the most-rated movies with a rating of 4 or higher. It is precomputed to improve application performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "initial_size = 10\n",
    "\n",
    "# Determine most-reviewed movies with rating of 4 or higher\n",
    "title_suggs = (w_df[[\"MovieID\", \"WeightedRating\", \"# of Ratings\"]]\n",
    "                .groupby(\"MovieID\").mean())\n",
    "title_suggs = title_suggs[title_suggs[\"WeightedRating\"] >= 4]\n",
    "title_suggs.sort_values(by=[\"# of Ratings\"], axis=0,\n",
    "                         ascending=False, inplace=True)\n",
    "title_suggs = title_suggs.iloc[:initial_size, :]\n",
    "title_suggs.index = \"m\" + title_suggs.index.astype(str)\n",
    "title_suggs = title_suggs.index.tolist()\n",
    "\n",
    "# Save to file\n",
    "with open(\"initial_suggestions.ob\", \"wb\") as fp:\n",
    "    pickle.dump(title_suggs, fp)\n",
    "\n",
    "print(f\"Initial suggestions:\\n{title_suggs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application\n",
    "\n",
    "We demonstrate our implementation of System I and System II in [our web application](https://psl-2023-project4.streamlit.app/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
