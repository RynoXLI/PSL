{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4: Movie Recommender System\n",
    "\n",
    "CS 598 Practical Statistical Learning\n",
    "\n",
    "2023-12-10\n",
    "\n",
    "UIUC Fall 2023\n",
    "\n",
    "**Authors**\n",
    "* Ryan Fogle\n",
    "    - rsfogle2@illinois.edu\n",
    "    - UIN: 652628818\n",
    "* Sean Enright\n",
    "    - seanre2@illinois.edu\n",
    "    - UIN: 661791377\n",
    "\n",
    "**Contributions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_dir = Path('.') / 'ml-1m' / 'ml-1m' \n",
    "ratings = pd.read_csv(movie_dir / 'ratings.dat', sep='::', engine = 'python', header=None)\n",
    "ratings.columns = ['UserID', 'MovieID', 'Rating', 'Timestamp']\n",
    "movies = pd.read_csv(movie_dir / 'movies.dat', sep='::', engine = 'python',\n",
    "                     encoding=\"ISO-8859-1\", header = None)\n",
    "movies.columns = ['MovieID', 'Title', 'Genres']\n",
    "users = pd.read_csv(movie_dir / 'users.dat', sep='::', engine='python', header=None)\n",
    "users.columns = ['UserID', 'Gender', 'Age', 'Occupation', 'Zipcode']\n",
    "\n",
    "# Create new entry for each genre in a movie, join movie and ratings together. \n",
    "movies['Genres'] = movies['Genres'].str.split('|')\n",
    "df = movies.explode('Genres')\n",
    "df = df.merge(ratings, on=['MovieID'], how='left')\n",
    "df.rename(columns={'Genres': 'Genre'}, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a MovieID -> Title map for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mov_title_map = dict(zip(movies['MovieID'], movies['Title']))\n",
    "list(mov_title_map.items())[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System I: Recommendation Based on Genres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General idea: Use Bayseian probabilty to calcuate new ratings based upon additional prior assumptions, and then rank movies by this new rating to select the top 5 per genre.\n",
    "\n",
    "This algorthm is based upon this stack overflow post:\n",
    "https://stackoverflow.com/questions/2495509/how-to-balance-number-of-ratings-versus-the-ratings-themselves\n",
    "\n",
    "$$\n",
    "\\tilde{R} = \\frac{\\bar{w} \\bar{r} + \\sum_{i=1}^{n}{r_i}}{\\bar{w} + n}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\tilde{R}$ is the new rating\n",
    "- $\\bar{w}$ is the predefined number of ratings (weight) to include in our prior assumption\n",
    "- $\\bar{r}$ is the predefined average rating to include in our prior assumption\n",
    "- $n$ is the number of ratings\n",
    "- $r_i$ is the rating for a given entry.\n",
    "\n",
    "**Interpretation**:\n",
    "\n",
    "Consider $\\bar{w}$ to be the average number of ratings for a given genre, and $\\bar{r}$ to be the number of times to consider that rating for a given genre. We first assume the rating of a movie to be defined as $\\frac{\\bar{w} \\bar{r}}{\\bar{w}}$ when $n=0$, then slightly update the estimate for each new given rating. \n",
    "\n",
    "In our implementation we defined $\\bar{w}$ to be the median genre rating from the average movie ratings in that genre. We also define $\\bar{r}$ to be the genre's 25th percentile count (of ratings per movie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by Genre and Movie, this will be used to find median ratings and percentile counts for our prior\n",
    "gb = df.groupby(['Genre', 'MovieID'])\n",
    "\n",
    "# Median Genre ratings of Average Movie ratings\n",
    "median_ratings = gb['Rating'].mean().reset_index().groupby('Genre')['Rating'].median().reset_index()\n",
    "median_ratings = dict(zip(median_ratings['Genre'], median_ratings['Rating']))\n",
    "median_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab 25th Percentile of count by genre\n",
    "quantile_count = gb['Timestamp'].count().reset_index().groupby('Genre')['Timestamp'].quantile(0.25).reset_index()\n",
    "quantile_count.columns = ['Genre', 'Count']\n",
    "quantile_count = dict(zip(quantile_count['Genre'], quantile_count['Count']))\n",
    "quantile_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_ratings = []\n",
    "for (genre, movie_id), movie in gb:\n",
    "    n = movie.shape[0]\n",
    "    w = quantile_count[genre]\n",
    "    r = median_ratings[genre] \n",
    "    weighted_rating = (r * w + movie['Rating'].sum()) / (w + n)\n",
    "    weighted_ratings.append((genre, movie_id, mov_title_map[movie_id], weighted_rating, movie['Rating'].sum() / n, n))\n",
    "\n",
    "w_df = pd.DataFrame(weighted_ratings, columns=['Genre', 'MovieID', 'Title', 'WeightedRating', 'AverageRating', '# of Ratings'])\n",
    "w_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort ratings by WeightedRating, group by genre and grab the first five occurrences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sysI_recs = w_df.sort_values('WeightedRating', ascending=False).groupby('Genre').head(n=10).sort_values(['Genre', 'WeightedRating'], ascending=[True, False])\n",
    "sysI_recs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output System I recommends for the dashboard to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sysI_recs.to_csv('sysI_recs.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System II: Recommendation Based on IBCF\n",
    "\n",
    "### Similarity Matrix Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_mov_df = pd.read_csv('Rmat.csv')\n",
    "user_mov_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine similarity definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def cosine_similarity(x, min_cardinality=3):\n",
    "    # Center by row mean\n",
    "    x -= np.nanmean(x, axis=1).reshape(-1, 1)\n",
    "    \n",
    "    # Prepare symmetric result matrix\n",
    "    s = np.empty((x.shape[1], x.shape[1]))\n",
    "    s[:] = np.nan\n",
    "\n",
    "    # Calculate similarity for upper trianglular matrix\n",
    "    for i in tqdm(range(0, x.shape[1] - 1)):\n",
    "        i_valid = ~np.isnan(x[:, i])\n",
    "        for j in range(i + 1, x.shape[1]):\n",
    "            j_valid = ~np.isnan(x[:, j])\n",
    "            row_mask = np.logical_and(i_valid, j_valid)\n",
    "            if row_mask.sum() >= min_cardinality:\n",
    "                r_li = x[row_mask, i]\n",
    "                r_lj = x[row_mask, j]\n",
    "                s[i, j] = (np.dot(r_li, r_lj)\n",
    "                           / (np.sqrt(np.power(r_li, 2).sum()) \n",
    "                              * np.sqrt(np.power(r_lj, 2).sum())))\n",
    "    s = 0.5 + s / 2\n",
    "\n",
    "    # Transpose upper triangular matrix to form lower\n",
    "    lower_idx = np.tril_indices(x.shape[1])\n",
    "    s[lower_idx] = s.T[lower_idx]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract data from dataframe and reformat as symmetric matrix of movie similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_cardinality = 3\n",
    "\n",
    "index = user_mov_df.columns\n",
    "s = cosine_similarity(user_mov_df.to_numpy(), min_cardinality=min_cardinality)\n",
    "s = pd.DataFrame(data=s,\n",
    "                 index=user_mov_df.columns,\n",
    "                 columns=user_mov_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter similarity matrix to only include top 30 similar movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_similar = 30\n",
    "\n",
    "for i in range(s.shape[0]):\n",
    "    row = s.iloc[i, :]\n",
    "    num_selected = min([(~np.isnan(row)).sum(), max_similar, len(row)])\n",
    "    # Find max allowed similarity with NaN vals\n",
    "    max_sim = np.roll(np.sort(row)[::-1], -np.count_nonzero(np.isnan(row)))[num_selected - 1]\n",
    "    na_mask = row < max_sim\n",
    "    s.iloc[i, na_mask] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write filtered similarity matrix to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.to_csv(\"similarity.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
