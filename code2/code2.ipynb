{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding Assignment 2\n",
    "\n",
    "**Authors**\n",
    "* Ryan Fogle\n",
    "    - rsfogle2@illinois.edu\n",
    "    - UIN: 652628818\n",
    "* Sean Enright\n",
    "    - seanre2@illinois.edu\n",
    "    - UIN: 661791377\n",
    "\n",
    "**Contributions**\n",
    "\n",
    "Ryan created the notebook and established the outline of the report, using the provided started code. In Part 1, he implemented `one_var_lasso()` in and provided an implementation of `MyLasso()` using sklearn scaling. He provided the initial implementation of the Part 2 simulation for all models in Case I, as well as generating the strip and box plots to summarize them graphically.\n",
    "\n",
    "Sean adapted `MyLasso()` to use Numpy array operations to perform the offsetting and normalization, as well as transformation back to the original scale. He performed troubleshooting of `MyLasso()` to test and report its accuracy. He refactored the Part 2 simulation code to apply to both Case I and Case II of the simulation. He reported the statistics for each model.\n",
    "\n",
    "Both team members contributed to the overall editing of the notebook, as well as the interpretation of the results in Part 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Implement Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed to the last four digits of our UINs\n",
    "np.random.seed(8818 + 1377)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myData = pd.read_csv(\"Coding2_Data.csv\")\n",
    "var_names = myData.columns\n",
    "y = myData[['Y']].to_numpy()\n",
    "X = myData.drop(['Y'], axis = 1).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CD for Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.typing as npt\n",
    "\n",
    "def one_var_lasso(r: npt.NDArray, x: npt.NDArray, lam):\n",
    "    \n",
    "    #################\n",
    "    # Your CODE\n",
    "    #################\n",
    "    \n",
    "    # x == z\n",
    "    # v == r\n",
    "\n",
    "    z2 = (x.T @ x).sum()\n",
    "    a = r.T @ x / z2\n",
    "    n = 2 * x.shape[0] * lam / z2\n",
    "\n",
    "    if a > n/2:\n",
    "        return a - n/2\n",
    "    elif np.abs(a) <= n/2:\n",
    "        return 0\n",
    "    elif a < -n/2:\n",
    "        return a + n/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def MyLasso(X, y, lam_seq, maxit = 100):\n",
    "    \n",
    "    # Input\n",
    "    # X: n-by-p design matrix without the intercept \n",
    "    # y: n-by-1 response vector \n",
    "    # lam.seq: sequence of lambda values (arranged from large to small)\n",
    "    # maxit: number of updates for each lambda \n",
    "    \n",
    "    # Output\n",
    "    # B: a (p+1)-by-len(lam.seq) coefficient matrix \n",
    "    #    with the first row being the intercept sequence \n",
    "\n",
    "  \n",
    "    n, p = X.shape\n",
    "    nlam = len(lam_seq)\n",
    "    B = np.zeros((p+1, nlam))\n",
    "    \n",
    "    ##############################\n",
    "    # YOUR CODE: \n",
    "    # (1) newX = Standardizad X; \n",
    "    # (2) Record the centers and scales used in (1) \n",
    "    ##############################\n",
    "    \n",
    "    # Centers\n",
    "    y_mean = y.mean()\n",
    "    X_mean = X.mean(axis=0)\n",
    "\n",
    "    # Scale\n",
    "    X_se = X.std(axis=0)\n",
    "\n",
    "    # Centering and scaling of X\n",
    "    newX = (X - X_mean) / X_se\n",
    "\n",
    "    # Initilize coef vector b and residual vector r\n",
    "    b = np.zeros(p)\n",
    "    r = y\n",
    "    \n",
    "    # Triple nested loop\n",
    "    for m in range(nlam):\n",
    "        for step in range(maxit):\n",
    "            for j in range(p):\n",
    "                X_j = newX[:, j].reshape(-1,1)\n",
    "                r = r + X_j * b[j]\n",
    "                b[j] = one_var_lasso(r, X_j, lam_seq[m])\n",
    "                r = r - X_j * b[j]\n",
    "        B[1:, m] = b \n",
    "    \n",
    "    ##############################\n",
    "    # YOUR CODE:\n",
    "    # Scale back the coefficients;\n",
    "    # Update the intercepts stored in B[, 1]\n",
    "    ##############################\n",
    "\n",
    "    # Scale back\n",
    "    B[1:, :] = B[1:, :] / X_se[:, np.newaxis]\n",
    "    \n",
    "    # Add in new intercepts\n",
    "    B[0, :] = y_mean - (X_mean[np.newaxis, :] @ B[1:, :]).squeeze()\n",
    "    \n",
    "    return(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_lam_seq = np.linspace(-1, -8, num = 80)\n",
    "lam_seq = np.exp(log_lam_seq)\n",
    "myout = MyLasso(X, y, lam_seq, maxit = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, _ = myout.shape\n",
    "plt.figure(figsize = (12,8))\n",
    "\n",
    "for i in range(p-1):\n",
    "    plt.plot(log_lam_seq, myout[i+1, :], label = var_names[i])\n",
    "\n",
    "plt.xlabel('Log Lambda')\n",
    "plt.ylabel('Coefficients')\n",
    "plt.title('Lasso Paths - Numpy implementation')\n",
    "plt.legend()\n",
    "plt.axis('tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the Accuracy\n",
    "The output of our algorithm is compared against the output from glmnet. The maximum difference between the two coefficient matrices should be lass than 0.005."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_coef = pd.read_csv(\"Coding2_lasso_coefs.csv\").to_numpy()\n",
    "lasso_coef.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs(myout - lasso_coef).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Simulation Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for Six Procedures\n",
    "\n",
    "The following code is shared by both Case I and Case II below.\n",
    "\n",
    "Six prediction procedures are defined:\n",
    "* Linear regression with all features\n",
    "* Ridge regression using `lambda.min`\n",
    "* Lasso with `lambda.min`\n",
    "* Lasso with `lambda.1se`\n",
    "* Lasso refit from the model with `lambda.1se`\n",
    "* Principal components regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge, RidgeCV, Lasso, LassoCV\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCR(object):\n",
    "\n",
    "    def __init__(self, num_folds=10):\n",
    "        self.folds = num_folds\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        n, p = X.shape\n",
    "        indices = np.arange(n)\n",
    "        np.random.shuffle(indices)\n",
    "        index_sets = np.array_split(indices, self.folds)\n",
    "        ncomp = min(p, n - 1 - max([len(i) for i in index_sets]))\n",
    "        cv_err = np.zeros(ncomp)\n",
    "\n",
    "        for ifold in range(self.folds):\n",
    "            train_inds =  np.delete(index_sets, obj=ifold, axis=0).ravel()\n",
    "            test_inds = index_sets[ifold]\n",
    "\n",
    "            X_train = X[train_inds, :]\n",
    "            pipeline = Pipeline([('scaling', StandardScaler()), ('pca', PCA())])\n",
    "            pipeline.fit(X_train)\n",
    "            X_train = pipeline.transform(X_train)\n",
    "            coefs = Y[train_inds].T @ X_train / np.sum(X_train**2, axis=0)\n",
    "            b0 = np.mean(Y[train_inds])\n",
    "\n",
    "            X_test = pipeline.transform(X[test_inds, :])\n",
    "\n",
    "            for k in np.arange(ncomp):\n",
    "                preds = X_test[:, :k] @ coefs.T[:k] + b0\n",
    "                cv_err[k] += cv_err[k] + np.sum((Y[test_inds]-preds)**2)\n",
    "\n",
    "        min_ind = np.argmin(cv_err)\n",
    "        self.ncomp = min_ind+1\n",
    "        pipeline = Pipeline([('scaling', StandardScaler()), ('pca', PCA(n_components=self.ncomp))])\n",
    "        self.transform = pipeline.fit(X)\n",
    "        self.model = LinearRegression().fit(self.transform.transform(X), Y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_ = self.transform.transform(X)\n",
    "        return self.model.predict(X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LassoBase:\n",
    "    \"\"\"Predict with Lasso Regression variants\n",
    "        Note: X_train and X_test must be centered and scaled\"\"\"\n",
    "    def __init__(self, X_train, y_train, n_jobs=6):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "        # Lasso\n",
    "        lasso_alphas = np.logspace(-10, 1, 100)\n",
    "        lassocv = LassoCV(alphas = lasso_alphas, cv = 10, n_jobs=n_jobs)\n",
    "        lassocv.fit(X_train, y_train)\n",
    "        cv_alphas = lassocv.alphas_\n",
    "        mean_mse = np.mean(lassocv.mse_path_, axis=1)\n",
    "        min_idx = np.argmin(mean_mse)\n",
    "\n",
    "        # Minimum\n",
    "        self.alpha_min = cv_alphas[min_idx]\n",
    "\n",
    "        # 1se\n",
    "        std_mse = np.std(lassocv.mse_path_, axis=1) / np.sqrt(10) \n",
    "        threshold = mean_mse[min_idx] + std_mse[min_idx]\n",
    "        self.alpha_1se = max(cv_alphas[np.where(mean_mse <= threshold)])\n",
    "\n",
    "        # Refit\n",
    "        self.nonzero_indices = None\n",
    "    \n",
    "    def min(self, X_test):\n",
    "        \"\"\"Regression with lambda.min\"\"\"\n",
    "        lasso_model_min = Lasso(alpha = self.alpha_min, max_iter=10000)\n",
    "        lasso_model_min.fit(self.X_train, self.y_train)\n",
    "        return lasso_model_min.predict(X_test)\n",
    "    \n",
    "    def one_se(self, X_test):\n",
    "        \"\"\"Regression with lambda.1se\"\"\"\n",
    "        lasso_model_1se = Lasso(alpha = self.alpha_1se, max_iter=10000)\n",
    "        lasso_model_1se.fit(self.X_train, self.y_train)\n",
    "        self.nonzero_indices = np.where(lasso_model_1se.coef_ != 0)[0]\n",
    "        return lasso_model_1se.predict(X_test)\n",
    "    \n",
    "    def refit(self, X_test):\n",
    "        \"\"\"Refit regression from lambda.1se\"\"\"\n",
    "        lm_refit = LinearRegression()\n",
    "        lm_refit.fit(self.X_train.iloc[:, self.nonzero_indices],\n",
    "                     self.y_train)\n",
    "        return lm_refit.predict(X_test.iloc[:, self.nonzero_indices])\n",
    "\n",
    "\n",
    "def full_model(X_train, y_train, X_test):\n",
    "    \"\"\"Predict with Full Linear Model\"\"\"\n",
    "    full = LinearRegression()\n",
    "    full.fit(X_train, y_train)\n",
    "    return full.predict(X_test)\n",
    "\n",
    "def ridge_regression(X_train, y_train, X_test):\n",
    "    \"\"\"Predict with Ridge Regression\n",
    "        Note: X_train and X_test must be centered and scaled\"\"\"\n",
    "    # Ridge regression\n",
    "    ridge_alphas = np.logspace(-10, 1, 100)\n",
    "    ridgecv = RidgeCV(alphas = ridge_alphas, cv = 10,\n",
    "                      scoring = 'neg_mean_squared_error')\n",
    "    ridgecv.fit(X_train, y_train)\n",
    "    ridge_model = Ridge(alpha = ridgecv.alpha_)\n",
    "    ridge_model.fit(X_train, y_train)\n",
    "    return ridge_model.predict(X_test)\n",
    "\n",
    "def principal_component_regression(X_train, y_train, X_test):\n",
    "    # perform PCR and train linear model.\n",
    "    pcr = PCR()\n",
    "    pcr.fit(X_train.to_numpy(), y_train.to_numpy())\n",
    "    return pcr.predict(X_test.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sim_data(X, Y, pct_test=0.25):\n",
    "    n = len(Y)\n",
    "    indices = np.arange(0, n)\n",
    "    np.random.shuffle(indices)\n",
    "    test_ind = indices[:int(np.floor(pct_test * n))]\n",
    "    train_ind = indices[len(test_ind):]\n",
    "\n",
    "    # Splitting the data into training and testing sets\n",
    "    X_train = X.iloc[train_ind]\n",
    "    y_train = Y[train_ind]\n",
    "    X_test = X.iloc[test_ind]\n",
    "    y_test = Y[test_ind]\n",
    "\n",
    "    # We need to scale data for Ridge and Lasso because they cannot normalize like R. \n",
    "    # Scaler\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scale = scaler.fit_transform(X_train)\n",
    "    X_test_scale = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, X_train_scale, X_test_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case I\n",
    "\n",
    "This simulation uses the data in `Coding2_Data2.csv`, which has 91 columns (1 response and 90 predictors), and 506 observations. The first 14 columns are the same data used in Part I."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/liangfgithub/liangfgithub.github.io/master/Data/Coding2_Data2.csv\"\n",
    "myData = pd.read_csv(url)\n",
    "Y = myData['Y']\n",
    "X = myData.drop(['Y'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, len(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulation\n",
    "\n",
    "In this simulation, each of the six models is used to for prediction with the `Coding2_Data2.csv` data, and their error is compared via mean squared prediction error (MSPE). The following procedure is carried out for 50 iterations:\n",
    "1) Partition data into 75% training and 25% test sets\n",
    "2) Fit training data with all six models, including:\n",
    "    * Linear regression with all features\n",
    "    * Ridge regression using `lambda.min`\n",
    "    * Lasso with `lambda.min`\n",
    "    * Lasso with `lambda.1se`\n",
    "    * Lasso refit from the model with `lambda.1se`\n",
    "    * Principal components regression\n",
    "3) Predict response with test data\n",
    "4) Evaluate MSPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# from sklearn.utils.testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "\n",
    "# Suppress convergence errors\n",
    "warnings.filterwarnings(action='ignore', category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_models = 6\n",
    "n_sims = 50\n",
    "data = []\n",
    "n_jobs = 6\n",
    "\n",
    "for i in tqdm(range(n_sims), total=n_sims):\n",
    "    X_train, y_train, X_test, y_test, X_train_scale, X_test_scale = gen_sim_data(X, Y,\n",
    "                                                                                 pct_test=0.25)\n",
    "    lasso = LassoBase(X_train, y_train, n_jobs=n_jobs)\n",
    "    predictions = (\n",
    "        mean_squared_error(y_test, full_model(X_train, y_train, X_test)),\n",
    "        mean_squared_error(y_test, ridge_regression(X_train_scale, y_train, X_test_scale)),\n",
    "        mean_squared_error(y_test, lasso.min(X_test)),\n",
    "        mean_squared_error(y_test, lasso.one_se(X_test)),\n",
    "        mean_squared_error(y_test, lasso.refit(X_test)),\n",
    "        mean_squared_error(y_test, principal_component_regression(X_train, y_train, X_test))\n",
    "    )\n",
    "    data.append(predictions)\n",
    "    \n",
    "case_i_df = pd.DataFrame(data, columns=['Full', 'Ridge.min', 'Lasso.min', 'Lasso.1se', 'L.Refit', 'PCR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistics\n",
    "\n",
    "The following statistics are reported for the simulation MPSE data:\n",
    "* Standard deviation (std)\n",
    "* Minimim (min)\n",
    "* Lower quartile (25%)\n",
    "* Median (50%)\n",
    "* Upper quartile (75%)\n",
    "* Maximum (max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_i_df.describe().drop(['count', 'mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graphical Summary\n",
    "\n",
    "The MSPE for each model is presented in the boxplot and strip charts below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Strip chart\n",
    "plt.title('MSPE by Model')\n",
    "sns.stripplot(case_i_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot\n",
    "plt.title('MSPE by Model')\n",
    "sns.boxplot(case_i_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "\n",
    "**Which procedure or procedures yield the best performance in terms of MSPE?**\n",
    "\n",
    "Ridge.min and Lasso.min yielded the best average MSPE.\n",
    "\n",
    "**Conversely, which procedure or procedures show the poorest performance?**\n",
    "\n",
    "PCR shows the worst performance.\n",
    "\n",
    "**In the context of Lasso regression, which procedure, Lasso.min or Lasso.1se, yields a better MSPE?**\n",
    "\n",
    "Lasso.min\n",
    "\n",
    "**Is refitting advantageous in this case? In other words, does L.Refit outperform Lasso.1se?**\n",
    "\n",
    "Yes, L.Refit outpreformed Lasso.1se.\n",
    "\n",
    "**Is variable selection or shrinkage warranted for this particular dataset? To clarify, do you find the performance of the Full model to be comparable to, or divergent from, the best-performing procedure among the other five?**\n",
    "\n",
    "No, the full model performance was comparable to that of the variable selection or shrinkge models. Therefore, the added benefit of doing variable selection is minimal. For this case, $n \\gg p$, so the full linear regression model has low variance and does not benefit much from regularization.\n",
    "\n",
    "[TODO: Maybe do a t-test/Anova on all prediction populations? ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case II\n",
    "\n",
    "This simulation uses the data in `Coding2_Data3.csv`, which has 591 columns (1 response and 590 predictors), and 506 observations. The first 92 columns are the same data used in Part II Case 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/liangfgithub/liangfgithub.github.io/master/Data/Coding2_Data3.csv\"\n",
    "myData = pd.read_csv(url)\n",
    "Y = myData['Y']\n",
    "X = myData.drop(['Y'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, len(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulation\n",
    "\n",
    "In this simulation, each of the six models is used to for prediction with the `Coding2_Data3.csv` data, and their error is compared via mean squared prediction error (MSPE). The following procedure is carried out for 50 iterations:\n",
    "1) Partition data into 75% training and 25% test sets\n",
    "2) Fit training data with five of the available models. The full linear regression model is excluded. The included models are:\n",
    "    * Ridge regression using `lambda.min`\n",
    "    * Lasso with `lambda.min`\n",
    "    * Lasso with `lambda.1se`\n",
    "    * Lasso refit from the model with `lambda.1se`\n",
    "    * Principal components regression\n",
    "3) Predict response with test data\n",
    "4) Evaluate MSPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_models = 5\n",
    "n_sims = 50\n",
    "data = []\n",
    "n_jobs = 6\n",
    "\n",
    "for i in tqdm(range(n_sims), total=n_sims):\n",
    "    X_train, y_train, X_test, y_test, X_train_scale, X_test_scale = gen_sim_data(X, Y,\n",
    "                                                                                 pct_test=0.25)\n",
    "    lasso = LassoBase(X_train, y_train, n_jobs=n_jobs)\n",
    "    predictions = (\n",
    "        mean_squared_error(y_test, ridge_regression(X_train_scale, y_train, X_test_scale)),\n",
    "        mean_squared_error(y_test, lasso.min(X_test)),\n",
    "        mean_squared_error(y_test, lasso.one_se(X_test)),\n",
    "        mean_squared_error(y_test, lasso.refit(X_test)),\n",
    "        mean_squared_error(y_test, principal_component_regression(X_train, y_train, X_test))\n",
    "    )\n",
    "    data.append(predictions)\n",
    "    \n",
    "case_ii_df = pd.DataFrame(data, columns=['Ridge.min', 'Lasso.min', 'Lasso.1se', 'L.Refit', 'PCR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistics\n",
    "\n",
    "The following statistics are reported for the simulation MPSE data:\n",
    "* Standard deviation (std)\n",
    "* Minimim (min)\n",
    "* Lower quartile (25%)\n",
    "* Median (50%)\n",
    "* Upper quartile (75%)\n",
    "* Maximum (max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_ii_df.describe().drop(['count', 'mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graphical Summary\n",
    "\n",
    "The MSPE for each model is presented in the boxplot and strip charts below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip chart\n",
    "plt.title('MSPE by Model')\n",
    "sns.stripplot(case_ii_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot\n",
    "plt.title('MSPE by Model')\n",
    "sns.boxplot(case_ii_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "\n",
    "**Which procedure or procedures yield the best performance in terms of MSPE?**\n",
    "\n",
    "Lasso.min, Lasso1.se and L.Refit show the best overall performance. Lasso.min in particular yields the lowest MPSE.\n",
    "\n",
    "**Conversely, which procedure or procedures show the poorest performance?**\n",
    "\n",
    "Ridge.min\n",
    "\n",
    "**Have you observed any procedure or procedures that performed well in Case I but exhibited poorer performance in Case II, or vice versa? If so, please offer an explanation.**\n",
    "\n",
    "Ridge.min performed much better in Case I than in Case II.\n",
    "\n",
    "In Case I, $n \\gg p$, so the ridge regression model reduces variance compared to the full model, resulting in superior performance. The fact that Ridge.min outperforms the Lasso variants indicates that most or all of the predictors are likely to be related to the response, so variable selection does not improve model performance as much as shrinkage.\n",
    "\n",
    "In Case II, $p \\gt n$, so the full model would have had very high variance an non-unique solutions. The Lasso variants and PCR show the best performance here on account of their variable selection behavior, which can reduce the predictors to a set with strong relationships to the response. The fact that the Lasso and PCR methods showed the best performance indicates that many of the predictors in Case II do not have strong relationships with the response.\n",
    "\n",
    "**Given that Coding2_Data3.csv includes all features found in Coding2_Data2.csv, one might anticipate that the best MSPE in Case II would be equal to or lower than the best MSPE in Case I. Do your simulation results corroborate this expectation? If not, please offer an explanation.**\n",
    "\n",
    "Yes, by adding more features, our MSPE went down."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
