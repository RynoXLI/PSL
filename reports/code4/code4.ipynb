{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding Assignment 4\n",
    "\n",
    "CS 598 Practical Statistical Learning\n",
    "\n",
    "2023-11-06\n",
    "\n",
    "UIUC Fall 2023\n",
    "\n",
    "**Authors**\n",
    "* Ryan Fogle\n",
    "    - rsfogle2@illinois.edu\n",
    "    - UIN: 652628818\n",
    "* Sean Enright\n",
    "    - seanre2@illinois.edu\n",
    "    - UIN: 661791377\n",
    "\n",
    "**Contributions**\n",
    "\n",
    "Part I:\n",
    "- Ryan contributed to implementing the E-step, Sean contributed to to refactoring and completely implementing the EM algorithm.\n",
    "\n",
    "\n",
    "Part II:\n",
    "- Sean implemented the Baum-Welch Algorithm, Ryan implemented the Viterbi Algorithm. \n",
    "\n",
    "## Part 1: Gaussian Mixtures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set random seed to the last four digits of our UINs\n",
    "np.random.seed(8818 + 1377 + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Estep(x: np.ndarray, G: int, pi: np.ndarray, mu: np.ndarray, sigma: np.ndarray):\n",
    "    \"\"\"EM algorithm expectation step. Here we estimate the latent variables based on the previous\n",
    "    estimates of theta to build a responsibility matrix.\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray): Data matrix, (n, p)\n",
    "        G (int): Number of classes\n",
    "        pi (np.ndarray): Mixing weights, (G,)\n",
    "        mu (np.ndarray): Mean values for each class, (p, G)\n",
    "        sigma (np.ndarray): Shared covariance matrix, (p, p)\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: The responsibility matrix of shape (n, G)\n",
    "    \"\"\"\n",
    "    resp = np.zeros((x.shape[0], G))\n",
    "    for k in range(G):\n",
    "        resp[:, k] = pi[k] * multivariate_normal_density(x, mu[:, k], sigma)\n",
    "    return resp / resp.sum(axis=1).reshape(-1, 1)\n",
    "\n",
    "def Mstep(x: np.ndarray, G: int, resp: np.ndarray, mu: np.ndarray):\n",
    "    \"\"\"EM algorithm maximization step.\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray): Data matrix, (n, p)\n",
    "        G (int): Number of classes\n",
    "        resp (np.ndarray): Responsibility matrix, (n, G)\n",
    "        mu (np.ndarray): Mean values per dimension, (p, G)\n",
    "    \n",
    "    Returns:\n",
    "        pi_new (np.ndarray): Updated mixing weights, (G,)\n",
    "        mu_new (np.ndarray): Updated mean values per dimension, (p, G)\n",
    "        sigma_new (np.ndarray): Updated covariance matrix, (p, p)\n",
    "    \"\"\"\n",
    "    n = x.shape[0]\n",
    "    # Pi\n",
    "    pi_new = resp.sum(axis=0) / n\n",
    "    # Mu\n",
    "    mu_new = (x.T @ resp) / resp.sum(axis=0)\n",
    "    # Sigma\n",
    "    sigma_new = np.zeros(sigma.shape)\n",
    "    for k in range(G):\n",
    "        tmp = x.T - mu_new[:, k].reshape(-1, 1)\n",
    "        #sigma_new += pi_new[k] * (resp[:, k] * A_mu) @ A_mu.T / resp[:, k].sum()\n",
    "        sigma_new += pi_new[k] * tmp @ np.diag(resp[:, k]) @ tmp.T / resp[:, k].sum()\n",
    "    return pi_new, mu_new, sigma_new\n",
    "\n",
    "def loglik(x: np.ndarray, G: int, pi: np.ndarray, mu: np.ndarray, sigma: np.ndarray):\n",
    "    \"\"\"Calculate log likelihood, given distribution parameters.\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray): Input data, shape (n, p)\n",
    "        G (int): Number of classes\n",
    "        pi (np.ndarray): Mixing weights, shape (G,)\n",
    "        mu (np.ndarray): Distribution means for each class, shape (p, G)\n",
    "        sigma (np.ndarray): Shared covariance matrix, shape (p, p)\n",
    "\n",
    "    Returns:\n",
    "        float: log likelihood\n",
    "    \"\"\"\n",
    "    ll = np.zeros(x.shape[0])\n",
    "    for k in range(G):\n",
    "        ll += pi[k] * multivariate_normal_density(x, mu[:, k], sigma)\n",
    "    return np.log(ll).sum()\n",
    "\n",
    "def multivariate_normal_density(x: np.ndarray, mu_k: np.ndarray, sigma: np.ndarray):\n",
    "    \"\"\"Evaluate multivariate normal probability density.\n",
    "    This is used in the E-step and in the log-likelihood calculation.\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray): data, shape (n, p)\n",
    "        mu_k (np.ndarray): mean for a given class, shape (p,)\n",
    "        sigma (np.ndarray): covariance matrix, shape (p, p)\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: n-dimensional probability densities\n",
    "    \"\"\"\n",
    "    A_mu = x.T - mu_k.reshape(-1, 1)\n",
    "    exponent = - 0.5 * np.multiply(A_mu, np.linalg.inv(sigma) @ A_mu).sum(axis=0)\n",
    "    return 1 / (2 * np.pi * np.sqrt(np.linalg.det(sigma))) * np.exp(exponent)\n",
    "\n",
    "def myEM(data: np.ndarray, G: int, prob: np.ndarray,\n",
    "         mean: np.ndarray, Sigma: np.ndarray, itmax: int):\n",
    "    \"\"\"Main EM algorithm\n",
    "\n",
    "    Args:\n",
    "        data (np.ndarray): Input data, shape (n, p)\n",
    "        G (int): Number of classes\n",
    "        prob (np.ndarray): Mixing weights, shape (G,)\n",
    "        mean (np.ndarray): Distribution means for each class, shape (p, G)\n",
    "        Sigma (np.ndarray): Shared covariance matrix, shape (p, p)\n",
    "        itmax (int): Number of EM iterations to perform\n",
    "\n",
    "    Returns:\n",
    "        (np.ndarray, np.ndarray, np.ndarray, float): probability vector, means, covariance and\n",
    "                                                     log-likelihood\n",
    "    \"\"\"\n",
    "    for _ in range(itmax):\n",
    "        resp = Estep(data, G, prob, mean, Sigma)\n",
    "        prob, mean, Sigma = Mstep(data, G, resp, mean)\n",
    "        ll = loglik(data, G, prob, mean, Sigma)\n",
    "    return prob, mean, Sigma, ll    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in data\n",
    "data = pd.read_csv('faithful.dat', header=0, sep='\\s+')\n",
    "data.head()\n",
    "data = data.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 1: G=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case G=2\n",
      "prob\n",
      "[0.04297883 0.95702117]\n",
      "\n",
      "mean\n",
      "[[ 3.49564188  3.48743016]\n",
      " [76.79789154 70.63205853]]\n",
      "\n",
      "Sigma\n",
      "[[  1.29793612  13.92433626]\n",
      " [ 13.92433626 182.58009247]]\n",
      "\n",
      "loglik\n",
      "-1289.5693549424109\n",
      "\n"
     ]
    }
   ],
   "source": [
    "G = 2\n",
    "n = data.shape[0]\n",
    "p1 = 10 / n\n",
    "p2 = 1 - p1\n",
    "mu1 = data[:10, :].mean(axis=0).reshape(-1, 1)\n",
    "mu2 = data[10:, :].mean(axis=0).reshape(-1, 1)\n",
    "\n",
    "sigma = 1 / n * (\n",
    "           (data[:10].T - mu1) @ (data[:10].T - mu1).T + \\\n",
    "           (data[10:].T - mu2) @ (data[10:].T - mu2).T\n",
    "        )\n",
    "\n",
    "pi = np.array((p1, p2)) # Shape (G,)\n",
    "mu = np.column_stack((mu1, mu2)) # Shape (p, G)\n",
    "\n",
    "prob, mean, Sigma, ll = myEM(data, G, pi, mu, sigma, 20)\n",
    "print(\"Case G=2\")\n",
    "print(f\"prob\\n{prob}\\n\\nmean\\n{mean}\\n\\nSigma\\n{Sigma}\\n\\nloglik\\n{ll}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 2: G=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case G=3\n",
      "prob\n",
      "[0.04363422 0.07718656 0.87917922]\n",
      "\n",
      "mean\n",
      "[[ 3.51006918  2.81616674  3.54564083]\n",
      " [77.10563811 63.35752634 71.25084801]]\n",
      "\n",
      "Sigma\n",
      "[[  1.26015772  13.51153756]\n",
      " [ 13.51153756 177.96419105]]\n",
      "\n",
      "loglik\n",
      "-1289.350958862739\n",
      "\n"
     ]
    }
   ],
   "source": [
    "G = 3\n",
    "p1 = 10 / n\n",
    "p2 = 20 / n\n",
    "p3 = 1 - p1 - p2\n",
    "mu1 = data[:10, :].mean(axis=0).reshape(-1, 1)\n",
    "mu2 = data[10:30, :].mean(axis=0).reshape(-1, 1)\n",
    "mu3 = data[30:, :].mean(axis=0).reshape(-1, 1)\n",
    "sigma = 1 / n * (\n",
    "           (data[:10].T - mu1) @ (data[:10].T - mu1).T + \\\n",
    "           (data[10:30].T - mu2) @ (data[10:30].T - mu2).T + \\\n",
    "           (data[30:].T - mu3) @ (data[30:].T - mu3).T\n",
    "        )\n",
    "\n",
    "pi = np.array((p1, p2, p3)) # Shape (G,)\n",
    "mu = np.column_stack((mu1, mu2, mu3)) # Shape (p, G)\n",
    "\n",
    "prob, mean, Sigma, ll = myEM(data, G, pi, mu, sigma, 20)\n",
    "print(\"Case G=3\")\n",
    "print(f\"prob\\n{prob}\\n\\nmean\\n{mean}\\n\\nSigma\\n{Sigma}\\n\\nloglik\\n{ll}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: HMM\n",
    "\n",
    "### Baum-Welch Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: the 2-by-2 transition matrix\n",
      "\n",
      "[[0.49793938 0.50206062]\n",
      " [0.44883431 0.55116569]]\n",
      "\n",
      "B: the 2-by-3 emission matrix\n",
      "\n",
      "[[0.22159897 0.20266127 0.57573976]\n",
      " [0.34175148 0.17866665 0.47958186]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def BW_onestep(data: np.ndarray, mx: np.ndarray, mz: np.ndarray,\n",
    "               w: np.ndarray, A: np.ndarray, B: np.ndarray):\n",
    "    \"\"\"Perform one iteration of the Baum-Welch algorithm, improving the estimates of\n",
    "       the transition probability and emission distribution matrices.\n",
    "\n",
    "    Args:\n",
    "        data (np.ndarray): Observations\n",
    "        mx (int): Count of distinct values X can take\n",
    "        mz (int): Count of distinct values Z can take\n",
    "        w (np.ndarray): An mz-by-1 probability vector representing the initial distribution for Z1.\n",
    "        A (np.ndarray): The mz-by-mz transition probability matrix that\n",
    "                        models the progression from Zt to Zt+1\n",
    "        B (np.ndarray): The mz-by-mx emission probability matrix,\n",
    "                        indicating how X is produced from Z\n",
    "\n",
    "    Returns:\n",
    "        (np.ndarray, np.ndarray): Updated A and B matrices\n",
    "    \"\"\"\n",
    "    n = data.shape[0]\n",
    "    \n",
    "    # E-step\n",
    "    # ==========================================================\n",
    "\n",
    "    # Forward algorithm\n",
    "    # Alpha is an mz-by-T forward probability matrix\n",
    "    alpha = np.empty((mz, n))\n",
    "    # \\alpha_1(i) = w(i) B(i, x_1)\n",
    "    alpha[:, 0] = np.multiply(w, B[:, data[0]])\n",
    "    for t in range(n - 1):\n",
    "        # \\alpha_{t+1}(i) = \\sum_j \\alpha_t(j) A(j,i) B(i, x_{t+1})\n",
    "        alpha[:, t + 1] = (A.T @ alpha[:, t]) * B[:, data[t + 1]]\n",
    "    \n",
    "    # Backward algorithm\n",
    "    # Beta is an mz-by-T backwards probability matrix\n",
    "    beta = np.empty((mz, n))\n",
    "    # \\beta_n(i) = 1\n",
    "    beta[:, n - 1] = 1\n",
    "    for t in np.arange(n - 2, -1, step = -1):\n",
    "        # \\beta{t}(i) = \\sum_j A(i, j) B(j, x_{t+1}) \\beta_{t+1}(j)\n",
    "        beta[:, t] = A @ (B[:, data[t + 1]] * beta[:, t + 1])\n",
    "\n",
    "    # Gamma\n",
    "    # \\gamma_t(i,j) = \\alpha_t(i) A(i, j) B(j, x_{t + 1}) \\beta_{t+1}(j)\n",
    "    gamma = np.empty((mz, mz, n - 1))\n",
    "    for t in range(n - 1):\n",
    "        for j in range(mz):\n",
    "            gamma[:, j, t] = alpha[:, t] * A[:, j] * B[j, data[t + 1]] * beta[j, t + 1]\n",
    "\n",
    "    # M-step\n",
    "    # ==========================================================\n",
    "\n",
    "    # Update A\n",
    "    A = gamma.sum(axis=2) # Sum over time\n",
    "    A /= A.sum(axis=1).reshape(-1, 1)\n",
    "\n",
    "    # Update B\n",
    "    # Marginalized gamma: mz-by-n\n",
    "    gamma_marginal = np.empty((mz, n))\n",
    "    # P(Z_t=i \\mid x) = \\sum_{j=1}^{m_z} P(Z_t=i, Z_{t+1} = j \\mid x) = \\sum_{j=1}^{m_z} \\gamma_t(i j)\n",
    "    gamma_marginal[:, :n - 1] = gamma.sum(axis=1)\n",
    "    # P(Z_t=i \\mid x) = \\sum_{j=1}^{m_z} P(Z_{t-1}=j, Z_t = i \\mid x) = \\sum_{j=1}^{m_z} \\gamma_{t-1}(j, i)\n",
    "    gamma_marginal[:, n - 1] = gamma[:, :, n - 2].sum(axis=0)\n",
    "    # B^*(i, l) = \\frac{\\sum_{t:x_t = l} \\gamma_t(i)} {\\sum_t \\gamma_t(i)}\n",
    "    for l in range(mx):\n",
    "        B[:, l] = gamma_marginal[:, data == l].sum(axis=1) / gamma_marginal.sum(axis=1)\n",
    "    return A, B\n",
    "\n",
    "def myBW(data: np.ndarray, mx: int, mz: int, w: np.ndarray,\n",
    "         A: np.ndarray, B: np.ndarray, itmax: int):\n",
    "    \"\"\"Perform the Baum-Welch algorithm for the Hidden Markov Model to estimate\n",
    "       the transition probability and emission distribution matrices.\n",
    "\n",
    "    Args:\n",
    "        data (np.ndarray): Observations\n",
    "        mx (int): Count of distinct values X can take\n",
    "        mz (int): Count of distinct values Z can take\n",
    "        w (np.ndarray): An mz-by-1 probability vector representing the initial distribution for Z1.\n",
    "        A (np.ndarray): The mz-by-mz transition probability matrix that\n",
    "                        models the progression from Zt to Zt+1\n",
    "        B (np.ndarray): The mz-by-mx emission probability matrix,\n",
    "                        indicating how X is produced from Z\n",
    "        itmax (int): Maximum number of EM step iterations to perform\n",
    "    \"\"\"\n",
    "    # Convert range of X values fron [1, 3] to  [0, 2] to facilitate indexing in Python\n",
    "    data = data - 1\n",
    "    for _ in range(itmax):\n",
    "        A, B = BW_onestep(data, mx, mz, w, A, B)\n",
    "    return A, B\n",
    "\n",
    "data = pd.read_csv('coding4_part2_data.txt', header=None).to_numpy().flatten()\n",
    "\n",
    "# Establish possible observations and number of latent states\n",
    "mx = np.unique(data).shape[0] # Unique X values\n",
    "mz = 2 # Given in instructions\n",
    "\n",
    "# Initialize transition probability and emission distribution matrices\n",
    "w = np.array((0.5, 0.5))\n",
    "A = np.full((2, 2), 0.5)\n",
    "B = np.row_stack([np.array([1, 3, 5]) / 9,\n",
    "                  np.array([1, 2, 3]) / 6])\n",
    "\n",
    "# Perform Baum-Welch to find estimates of A and B\n",
    "A, B = myBW(data, mx, mz, w, A, B, 100)\n",
    "print(f\"A: the {mz}-by-{mz} transition matrix\\n\\n{A}\\n\\n\"\n",
    "      f\"B: the {mz}-by-{mx} emission matrix\\n\\n{B}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viterbi Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== Z Valid ========================\n",
      "\n",
      "[1 1 1 1 1 1 1 2 1 1 1 1 1 2 2 1 1 1 1 1 1 1 2 2 2 2 2 1 1 1 1 1 1 1 2 1 1\n",
      " 1 1 1 1 1 1 2 2 1 1 1 1 1 1 2 2 2 1 1 1 1 2 2 2 2 1 1 1 1 1 1 1 1 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1\n",
      " 1 1 1 2 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2\n",
      " 2 2 2 1 1 1 2 2 2 2 2 2 1 1 1 1 1 2 2 2 2 2 2 2 2 2 1 1 1 2 2 2 1 1 1 1 1\n",
      " 1 1 1 2 2 2 2 2 1 1 1 1 1 1 1] \n",
      "\n",
      "================== Z Calculated ===================\n",
      "\n",
      "[1 1 1 1 1 1 1 2 1 1 1 1 1 2 2 1 1 1 1 1 1 1 2 2 2 2 2 1 1 1 1 1 1 1 2 1 1\n",
      " 1 1 1 1 1 1 2 2 1 1 1 1 1 1 2 2 2 1 1 1 1 2 2 2 2 1 1 1 1 1 1 1 1 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1\n",
      " 1 1 1 2 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2\n",
      " 2 2 2 1 1 1 2 2 2 2 2 2 1 1 1 1 1 2 2 2 2 2 2 2 2 2 1 1 1 2 2 2 1 1 1 1 1\n",
      " 1 1 1 2 2 2 2 2 1 1 1 1 1 1 1] \n",
      "\n",
      "\n",
      "Z Valid == Z Calc: True\n"
     ]
    }
   ],
   "source": [
    "def myViterbi(data: np.ndarray, mx: int, mz: int, w: np.ndarray,\n",
    "         A: np.ndarray, B: np.ndarray, itmax: int):\n",
    "    \"\"\"Perform the Viterbi Algorithm to output the most likely latent sequence considering \n",
    "        the data and the MLE of the parameters.\n",
    "\n",
    "    Args:\n",
    "        data (np.ndarray): Observations\n",
    "        mx (int): Count of distinct values X can take\n",
    "        mz (int): Count of distinct values Z can take\n",
    "        w (np.ndarray): An mz-by-1 probability vector representing the initial distribution for Z1.\n",
    "        A (np.ndarray): The mz-by-mz transition probability matrix that\n",
    "                        models the progression from Zt to Zt+1\n",
    "        B (np.ndarray): The mz-by-mx emission probability matrix,\n",
    "                        indicating how X is produced from Z\n",
    "        itmax (int): Maximum number of EM step iterations to perform\n",
    "    \"\"\"\n",
    "\n",
    "    # Perform Baum-Welch to find estimates of A and B\n",
    "    A, B = myBW(data, mx, mz, w, A, B, itmax)\n",
    "\n",
    "    # put all valus on log-scale\n",
    "    w = np.log(w)\n",
    "    A = np.log(A)\n",
    "    B = np.log(B)\n",
    "\n",
    "    # initialize additional parameters\n",
    "    n = data.shape[0]\n",
    "    delta = np.zeros((mz, n))\n",
    "    Z = np.zeros(n, dtype=int)\n",
    "\n",
    "    # subtract 1 from data, python is indexed by 0 as the start.  \n",
    "    data = data - 1\n",
    "\n",
    "    # set initial delta value\n",
    "    delta[:, 0] = w + B[:, data[0]]\n",
    "\n",
    "    # update for delta\n",
    "    for idx in range(n - 1):\n",
    "        delta[:, idx + 1] = np.max(A + delta[:, idx].reshape(-1,1), axis=0) + B[:, data[idx + 1]]\n",
    "    # print(delta.T)\n",
    "    \n",
    "    # find optimal Z value. \n",
    "    Z[n-1] = np.argmax(delta[:, n-1])\n",
    "    for idx in range(n-1, 0, -1):\n",
    "        Z[idx - 1] = np.argmax(delta[:, idx-1] + A[:, Z[idx]])\n",
    "\n",
    "    # add one at the end to match output\n",
    "    return Z + 1\n",
    "\n",
    "data = pd.read_csv('coding4_part2_data.txt', header=None).to_numpy().flatten()\n",
    "\n",
    "# Establish possible observations and number of latent states\n",
    "mx = np.unique(data).shape[0] # Unique X values\n",
    "mz = 2 # Given in instructions\n",
    "\n",
    "# Initialize transition probability and emission distribution matrices\n",
    "w = np.array((0.5, 0.5))\n",
    "A = np.full((2, 2), 0.5)\n",
    "B = np.row_stack([np.array([1, 3, 5]) / 9,\n",
    "                  np.array([1, 2, 3]) / 6])\n",
    "\n",
    "# Load in valid Z values for comparison\n",
    "Z_valid = []\n",
    "with open('Coding4_part2_Z.txt', 'r') as f:\n",
    "    Z_valid = np.array(f.read().strip().split(' ')).astype(int)\n",
    "\n",
    "# Run Viterbi algorithm\n",
    "Z = myViterbi(data, mx, mz, w, A, B, 100)\n",
    "\n",
    "# Output results\n",
    "print('================== Z Valid ========================\\n')\n",
    "print(Z_valid, '\\n')\n",
    "print('================== Z Calculated ===================\\n')\n",
    "print(Z, '\\n')\n",
    "print('\\nZ Valid == Z Calc:', np.array_equal(Z, Z_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
